{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge svd?\n",
    "\n",
    "#np.roll idea needs adjustment for last fold, maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "import scipy.linalg as scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Franke function uglified\n",
    "def FrankeFunction(x,y): #still got one division in here\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)*(9*x-2)) - 0.25*((9*y-2)*(9*y-2)))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)*(9*x+1))/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)*(9*x-7)*0.25 - 0.25*((9*y-3)*(9*y-3)))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)*(9*x-4) - (9*y-7)*(9*y-7))\n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model matrix\n",
    "# Form is: x0y0, x1y0, x2y0, x0y1, x1y1, x0y2\n",
    "def Model(x,y,P): # P is polynomial degree\n",
    "    m = len(x)*len(y) # number of equations\n",
    "    t = sum(range(P+2)) # number of terms in polynomial\n",
    "    X = np.zeros((m,t)) # Model matrix\n",
    "    a = np.matrix.flatten(x)\n",
    "    b = np.matrix.flatten(y)\n",
    "    c = 0 #counter\n",
    "    for i in range(P+1):\n",
    "        for j in range(P+1-i):\n",
    "            X[:,c] = a**j*b**i\n",
    "            c +=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_svd(X: np.ndarray, z: np.ndarray,_lambda) -> np.ndarray:\n",
    "    u, s, v = scl.svd(X)\n",
    "    pin = scl.diagsvd(1./s,len(v),len(u))\n",
    "#     return v.T @ scl.pinv(scl.diagsvd(s, u.shape[0], v.shape[0])) @ u.T @ z\n",
    "    return v.T @ pin @ u.T @ z\n",
    "#pinx xt x\n",
    "# THIS ALGO NEEDS IMPROVEMENT! QR decomp\n",
    "\n",
    "#same results as skl up to and including mpd14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_inv(X: np.ndarray, z: np.ndarray) -> np.ndarray:\n",
    "    return scl.inv(X.T @ X) @ (X.T @ z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define R2 function\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define MSE function\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variance function\n",
    "def Var(y_data, y_model,P,X):\n",
    "    N = len(y_data)\n",
    "    covar = np.linalg.inv(X.T.dot(X))#sigma2, should be 1 in this case\n",
    "    vari = np.diagonal(covar)\n",
    "    return vari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Var2(y_data,y_model,n):\n",
    "    m = len(y_data)\n",
    "    res = y_data-y_model\n",
    "    return (1./(m-n-1))*(res.T@res)\n",
    "#     return np.mean(y_model**2)-np.mean(yh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bias(y_data, y_model, P, X):\n",
    "    N = len(y_data) #should probably insert z here\n",
    "    # also want the expectation values\n",
    "    #this is where i want to shuffle my data and do 10 kfolds.\n",
    "    #maybe...\n",
    "    #can not use each fold and create a mean from them?\n",
    "    return np.sum((y_data-y_model)*(y_data-y_model))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bias2(y_data,y_model):\n",
    "    n = len(y_data)\n",
    "    return np.sum((y_data-(np.mean(y_model)))**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create random variables/predictors\n",
    "# np.random.seed(1234)\n",
    "N = 20  #number of points along x and y axes\n",
    "\n",
    "# sort this for meshgrid\n",
    "x = np.random.uniform(0,1,N)\n",
    "y = np.random.uniform(0,1,N)\n",
    "\n",
    "x = sorted(x)\n",
    "y = sorted(y)\n",
    "x, y = np.meshgrid(x,y,sparse=False)\n",
    "\n",
    "#create datapoints/results\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "# Create noise\n",
    "Noise = 0.1*np.random.randn(N,N)\n",
    "\n",
    "#add noise\n",
    "z_n = z+Noise\n",
    "\n",
    "#flatten for use in functions\n",
    "z_n = np.matrix.flatten(z_n)\n",
    "z = np.matrix.flatten(z)\n",
    "\n",
    "#added a 0 Ridge parameter\n",
    "_lambda = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPD =5#maximal polynomial degree\n",
    "\n",
    "X = Model(x,y,MPD) #create model matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoResampling(X,z_n,_lambda):\n",
    "#     beta = np.linalg.inv(X.T.dot(X)-_lambda*np.eye(len(X[0][:]))).dot(X.T).dot(z_n)\n",
    "#     beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(z_n)\n",
    "    beta = ols_svd(X,z_n,_lambda)\n",
    "#     beta = ols_inv(X,z_n)\n",
    "    ztilde = X @ beta\n",
    "\n",
    "    return MSE(z_n,ztilde), R2(z_n,ztilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010953330663105687, 0.8743377206149485)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoResampling(X,z_n,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find parameters\n",
    "# beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(z_n)\n",
    "beta = ols_svd(X,z_n,_lambda)\n",
    "## make prediction\n",
    "ztilde = X @ beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Makes model matrix, looks different than mine #####\n",
    "# form: x0y0, x1y0, x0y1, x2y0, x1y1, x0y2\n",
    "# poly2 = PolynomialFeatures(degree=MPD)\n",
    "# a = x[:,np.newaxis]\n",
    "# b = y[:,np.newaxis]\n",
    "# cc = np.c_[a,b]\n",
    "# Xskl = poly2.fit_transform(cc) \n",
    "##########################################################\n",
    "# THIS CELL IS CURRENTLY NOT IN USE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find parameters\n",
    "clf = skl.LinearRegression(fit_intercept=False) #False to not center data, i.e. intercept is not 0\n",
    "clf.fit(X,z_n) \n",
    "\n",
    "#make prediction\n",
    "zpredict = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (self): 0.01095\n",
      "Mean squared error (skl): 0.01095\n",
      "R2 score (self): 0.87434\n",
      "R2 score (skl): 0.87434\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error              \n",
    "print(\"Mean squared error (self): %.5f\" % MSE(z_n, ztilde))\n",
    "print(\"Mean squared error (skl): %.5f\" % mean_squared_error(z_n, zpredict))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction     \n",
    "print('R2 score (self): %.5f' %R2(z_n, ztilde))\n",
    "print('R2 score (skl): %.5f' % r2_score(z_n, zpredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.37595934e+00 1.14261493e+02 2.40112552e+03 1.28726789e+04\n",
      " 1.44291418e+04 2.18571532e+03 3.61461887e+02 2.53556462e+03\n",
      " 6.01782767e+03 4.89384260e+03 8.66559213e+02 7.80593180e+03\n",
      " 1.14673676e+04 8.56265875e+03 1.03992207e+03 3.61366777e+04\n",
      " 1.02936988e+04 2.07883713e+03 3.53434180e+04 1.71350220e+03\n",
      " 4.80249246e+03]\n",
      "[2.37595934e+00 1.14261493e+02 2.40112552e+03 1.28726789e+04\n",
      " 1.44291418e+04 2.18571532e+03 3.61461887e+02 2.53556462e+03\n",
      " 6.01782767e+03 4.89384260e+03 8.66559213e+02 7.80593180e+03\n",
      " 1.14673676e+04 8.56265875e+03 1.03992207e+03 3.61366777e+04\n",
      " 1.02936988e+04 2.07883713e+03 3.53434180e+04 1.71350220e+03\n",
      " 4.80249246e+03]\n"
     ]
    }
   ],
   "source": [
    "print(Var(z_n, ztilde, MPD,X)) #my variances in betas\n",
    "print(Var(z_n,zpredict,MPD,X)) #skl variances in betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.27248503   7.89793843 -36.54968178  56.3501057  -34.38445574\n",
      "   6.48635214   6.11669947 -17.70109348  46.72441349 -52.7761958\n",
      "  17.75811044 -16.06396127  24.18640356 -14.35813375  11.97638168\n",
      "   3.80210871 -24.11352472  -2.39718131  17.14478568  10.70838136\n",
      " -11.08225171]\n",
      "[  0.27248503   7.89793843 -36.54968178  56.3501057  -34.38445574\n",
      "   6.48635214   6.11669947 -17.70109348  46.72441349 -52.7761958\n",
      "  17.75811044 -16.06396127  24.18640356 -14.35813375  11.97638168\n",
      "   3.80210871 -24.11352472  -2.39718131  17.14478568  10.70838136\n",
      " -11.08225171]\n"
     ]
    }
   ],
   "source": [
    "print(beta) #my betas\n",
    "print(clf.coef_) #skl betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2724850306318026 3.021172853149135\n",
      "7.897938427265337 20.95106085083251\n",
      "-36.5496817844621 96.04251043643849\n",
      "56.35010570156706 222.37734405981226\n",
      "-34.384455738280366 235.4378707661387\n",
      "6.4863521356560945 91.63320335730295\n",
      "6.1166994742845775 37.26381602822263\n",
      "-17.701093483150185 98.69460492438644\n",
      "46.72441348924867 152.04633099131036\n",
      "-52.77619579508718 137.1137692821534\n",
      "17.758110438273423 57.69726054372396\n",
      "-16.063961265456093 173.16832163351935\n",
      "24.186403561208802 209.88815928130876\n",
      "-14.358133745722899 181.36788543186978\n",
      "11.97638167955756 63.20573267263559\n",
      "3.802108705997348 372.5891314811888\n",
      "-24.113524722212283 198.8574191333822\n",
      "-2.397181314066359 89.3647620975117\n",
      "17.144785676478165 368.47696626515517\n",
      "10.708381358109937 81.13316256411738\n",
      "-11.082251705298152 135.82803474777634\n"
     ]
    }
   ],
   "source": [
    "std = 1.96*np.sqrt(Var(z_n, ztilde, MPD,X))\n",
    "\n",
    "for i in range(len(std)):\n",
    "    print(beta[i],std[i])\n",
    "#these things are huge, something is probably off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 21) (400,)\n"
     ]
    }
   ],
   "source": [
    "# 95% CI: mu +-1.96*sigma\n",
    "print(np.shape(X),np.shape(z_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksajjad(X,z,k):\n",
    "#     m,n = np.shape(X) #Swapped these two, ask sajjad\n",
    "    n,m = np.shape(X)\n",
    "    ind = np.arange(0,n,k)\n",
    "    for i in ind:\n",
    "        X_test, z_test = X[i:(i+k),:],z[i:(i+k)]\n",
    "        if i ==0:\n",
    "            X_train,z_train = X[i+k:,:],z[i+k:]\n",
    "        elif i ==(n-k):\n",
    "            X_train,z_train = X[:i,:],z[:i]\n",
    "        else:\n",
    "            X_train = np.vstack([X[:i,:],X[i+k:,:]])\n",
    "            z_train = np.hstack([z[:i],z[i+k:]])\n",
    "        yield X_train, X_test, z_train, z_test #yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ksajjad at 0x7fc19d494390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksajjad(X,z_n,5)\n",
    "# for XX,XXX,YY,YYY in ksajjad(X,z_n,5):\n",
    "#     print(\"bip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np roll method may include same points again in last roll,depending on numbers. do calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 # number of folds\n",
    "partition = 0.2 # percentage of data to use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(k,X,z_n):\n",
    "    #shuffle data before doing the kFold\n",
    "    N=20\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n]\n",
    "    np.random.shuffle(combi)\n",
    "    X, z_n = combi[:,:n], combi[:,n]\n",
    "    CV = []\n",
    "    partition = 0.2 # percentage of data to use for testing\n",
    "    sMSE = 0. #sum mean squared error\n",
    "    sR2 = 0. #sum R2 score\n",
    "    zz = []\n",
    "    tt = []\n",
    "    for X_train, X_test, z_train, z_test in ksajjad(X,z_n,40):\n",
    "\n",
    "        # find parameters\n",
    "        beta = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(z_train)\n",
    "#         beta = ols_svd(X_train,z_train,0.)\n",
    "\n",
    "        # make prediction\n",
    "        z_tilde = X_test @ beta\n",
    "        \n",
    "        zz.append(z_test)\n",
    "        tt.append(z_tilde)\n",
    "\n",
    "        CV.append(MSE(z_tilde,z_test))\n",
    "\n",
    "\n",
    "    zz = np.ravel(zz)\n",
    "    tt = np.ravel(tt)\n",
    "    order = np.argsort(zz)\n",
    "    tt = tt[order]\n",
    "    \n",
    "    return tt, CV\n",
    "\n",
    "#     return CV,bb\n",
    "#         return z_tilde\n",
    "#     print(np.shape(CV))\n",
    "#     print(CV[0][0],z_n[0])\n",
    "#     return CV\n",
    "#     return(MSE(np.matrix.flatten(np.array([CV])),z_n), R2(np.matrix.flatten(np.array([CV])),z_n),Var2(np.matrix.flatten(np.array([CV])),z_n,N),np.var(np.matrix.flatten(np.array([CV])),axis=0),Bias2(np.matrix.flatten(np.array([CV])),z_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08833755532926102\n",
      "0.08720465557084617\n",
      "6.73294755260533e-05\n",
      "0.0879097031359665\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean(kFold(k,X,z_n)))\n",
    "\n",
    "## SAJJAD: look at the article by mehta, page 36 for \n",
    "# bias variance tradeoff over multiple datasets\n",
    "\n",
    "#i tried returning the model values from the function above, \n",
    "# called tt, to be able to calculate bi/var stuff as Mehta.\n",
    "\n",
    "\n",
    "itera = []\n",
    "cost = []\n",
    "\n",
    "for i in range(10):\n",
    "    itera.append(kFold(k,X,z_n)[0])\n",
    "\n",
    "itera = np.array(itera)\n",
    "xi = np.mean(itera,axis=0)\n",
    "zi = sorted(z_n)\n",
    "\n",
    "for i in range(10):\n",
    "    cost.append(MSE(itera[i,:],zi))\n",
    "err = np.mean(cost)\n",
    "print(err)\n",
    "\n",
    "meann = np.mean(itera,axis=0)\n",
    "B2 = np.sum((z_n-meann)**2)\n",
    "print(B2/len(z_n))\n",
    "\n",
    "inni =(itera-meann)**2\n",
    "\n",
    "uti = np.mean(inni,axis=0)\n",
    "vr = np.sum(uti)\n",
    "print(vr/len(z_n))\n",
    "print(np.mean(kFold(k,X,z_n)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldskl(k,X,z_n):\n",
    "    from sklearn.model_selection import KFold\n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n]\n",
    "    np.random.shuffle(combi)\n",
    "    X, z_n = combi[:,:n], combi[:,n]\n",
    "    \n",
    "#     kfold = KFold(k,True,1)\n",
    "    kfold = KFold(k,False)\n",
    "    sMSE = 0\n",
    "    sR2 = 0\n",
    "    CV = []\n",
    "    b2 = 0\n",
    "    for train,test in kfold.split(X,z_n):\n",
    "        # find parameters\n",
    "        clf = skl.LinearRegression(fit_intercept=False) #False to not center data, i.e. intercept is not 0\n",
    "        clf.fit(X[train],z_n[train]) \n",
    "\n",
    "        #make prediction\n",
    "        zpredict = clf.predict(X[test])\n",
    "        CV.append(zpredict.T)\n",
    "#         print(np.shape(zpredict))\n",
    "        \n",
    "#         sMSE += mean_squared_error(z_n[test],zpredict)\n",
    "#         sR2 += r2_score(z_n[test],zpredict)\n",
    "#     print(np.shape(CV))\n",
    "#     return sMSE/k, sR2/k\n",
    "#     print(CV[0][0],z_n[0])\n",
    "    return(MSE(np.matrix.flatten(np.array([CV])),z_n), R2(np.matrix.flatten(np.array([CV])),z_n))\n",
    "#     return MSE(zpredict,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643.6368984086482\n"
     ]
    }
   ],
   "source": [
    "#function test\n",
    "a=0\n",
    "b=0\n",
    "c=0\n",
    "d=0\n",
    "b2 = 0\n",
    "itera = []\n",
    "zs = []\n",
    "k = 10\n",
    "# print(np.shape(kFold(k,X,z_n)[1]))\n",
    "uff = np.matrix.flatten(np.array([kFold(k,X,z_n)[1]]))\n",
    "# print(np.shape(uff))\n",
    "# print(kFold(k,X,z_n))\n",
    "# print(np.mean(kFold(k,X,z_n)))\n",
    "for i in range(10):\n",
    "    itera.append(np.mean(kFold(k,X,z_n)[0]))\n",
    "    zs.append(np.matrix.flatten(np.array([kFold(k,X,z_n)[1]])))\n",
    "    b2 += np.sum((z_n-np.mean(zs[i]))**2)\n",
    "print(b2)\n",
    "#     print(itera[i])\n",
    "#     print(z_n)\n",
    "#     b2 += np.sum((z_n-itera[i])**2)\n",
    "#     print(kFold(k,X,z_n))\n",
    "# print(np.mean(itera),np.std(itera),np.var(itera))\n",
    "# print(b2/100)\n",
    "# mi = np.mean(itera)\n",
    "# su =0.\n",
    "# for i in range(10):\n",
    "#     su +=(itera[i]-mi)**2\n",
    "# print(su/10.)\n",
    "# print(21./10.*0.009)\n",
    "# print(np.shape(kFold(k,X,z_n)))\n",
    "#     a1,b1 = kFold(k,X,z_n)\n",
    "#     a+=a1\n",
    "#     b+=b1\n",
    "#     c1,d1 = kFoldskl(k,X,z_n)\n",
    "#     c += c1\n",
    "#     d += d1\n",
    "# print(a/10,c/10,b/10,d/10)\n",
    "#     print(\"skl\",kFoldskl(k,X,z_n))\n",
    "#     kFold(k,X,z_n)\n",
    "#     kFoldskl(k,X,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRidge(k,X,z_n,_lambda):\n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n]\n",
    "    np.random.shuffle(combi)\n",
    "    X, z_n = combi[:,:n], combi[:,n]\n",
    "    \n",
    "    partition = 0.2 # percentage of data to use for testing\n",
    "    sMSE = 0 #sum mean squared error\n",
    "    sR2 = 0 #sum R2 score\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Split data\n",
    "        X_train,X_test,z_train,z_test = train_test_split(X,z_n,test_size=partition,shuffle=False)\n",
    "        \n",
    "        # find parameters\n",
    "#         beta = np.linalg.inv(X_train.T.dot(X_train)-_lambda*np.eye(len(X[0][:]))).dot(X_train.T).dot(z_train)\n",
    "\n",
    "        beta = np.linalg.inv(X_train.T.dot(X_train)-_lambda*np.eye(n)).dot(X_train.T).dot(z_train)\n",
    "\n",
    "        # make prediction\n",
    "        z_tilde = X_test @ beta\n",
    "\n",
    "        # sum of MSE and R2, remember to divide by k later\n",
    "        sMSE += MSE(z_test,z_tilde)\n",
    "        sR2 += R2(z_test,z_tilde)\n",
    "\n",
    "        # Prepare data for next fold\n",
    "        X = np.roll(X,len(X_test),axis=0) # Rolls matrix downwards before next split\n",
    "        z_n = np.roll(z_n,len(z_test)) # Roll\n",
    "    \n",
    "    return sMSE/k, sR2/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.012265031983984586, 0.8531054494832586)\n",
      "[ 6.58553608e-02  2.10245379e-02  1.37286245e-01  1.35261437e-01\n",
      "  1.62219821e-02  1.99704312e-02  9.85229430e-02  1.04823205e-02\n",
      "  5.65709356e-02  1.02261417e-02  1.85494408e-02  1.03479756e-01\n",
      "  1.35759234e-01  6.44605906e-03  7.49619689e-03  3.43742432e-02\n",
      "  5.62100939e-02  6.07884679e-02  3.58380831e-02  6.02620677e-02\n",
      "  1.20733359e-01  1.60821694e-01  1.49539823e-01  1.24879924e-01\n",
      " -1.97000202e-02  8.81192564e-02  8.36265460e-02  2.44538495e-02\n",
      "  8.44961657e-02  4.39572152e-02  5.47220308e-02  1.99947318e-03\n",
      "  6.03841908e-02  3.13205086e-02  3.55675188e-02  1.26080798e-02\n",
      "  1.27397146e-01  1.80602853e-01  5.50085486e-02  1.62560834e-01\n",
      " -1.89033330e-02  1.96264585e-01  1.47773021e-01  1.08983905e-01\n",
      "  1.71964055e-01  1.28675481e-01  1.26138929e-01  1.25163443e-01\n",
      "  1.53225392e-01 -3.53742165e-03  1.23008255e-01  1.02978287e-01\n",
      "  5.71432265e-02  1.28805350e-01 -1.52427069e-04  1.43922015e-01\n",
      "  1.78242772e-03  1.23489848e-01  3.55276779e-02  2.17410532e-01\n",
      "  1.24982159e-01  8.59948225e-02  1.55771189e-01  7.02181081e-02\n",
      "  2.78173997e-01  1.76487045e-01  1.29675576e-01  8.70708501e-02\n",
      "  2.00386000e-01  1.97997282e-01  7.11741785e-03  1.79147764e-01\n",
      "  1.22562317e-01  1.73176635e-01  1.20554247e-01  1.67532647e-01\n",
      "  1.62882914e-01  4.48602922e-02  8.51343151e-03  2.51128165e-01\n",
      " -1.16247076e-02  1.57350741e-01 -2.13550291e-03  1.15929293e-01\n",
      "  1.94119132e-02  2.74239810e-01  2.57087182e-01  1.39363163e-01\n",
      "  1.38206815e-02  7.92263583e-03  1.25372570e-01  1.81715022e-01\n",
      "  1.98908075e-01  7.16299251e-02  4.68900274e-02  1.50083488e-01\n",
      "  3.21470183e-01  1.65304356e-01  1.71577650e-01  1.16157442e-01\n",
      "  1.31162423e-01  1.38358485e-01  9.71698783e-02  8.58357507e-02\n",
      "  1.22559776e-01  1.14695929e-01  1.68972320e-01  1.12809547e-01\n",
      "  9.16198446e-02  2.23044564e-01  1.37922248e-01  3.17624189e-01\n",
      "  2.55744117e-01  1.62445507e-01  2.94643449e-01  2.30631727e-01\n",
      " -3.80855915e-03  1.96068862e-01  7.58555908e-02  1.28641777e-01\n",
      "  2.47055587e-02  2.46029281e-01  2.76719556e-01  1.79760251e-01\n",
      "  1.45320169e-01  8.75713591e-02  2.61159834e-01  7.53449223e-03\n",
      "  2.16984518e-01  1.26609967e-01  1.32435182e-01  2.44416019e-01\n",
      "  9.30088341e-02  1.61179597e-01 -1.18723811e-03  2.84211055e-02\n",
      " -1.61458374e-03  1.29639645e-01  9.59881728e-02  2.07192873e-01\n",
      "  7.11459815e-02  1.05036243e-01 -2.21444028e-03  1.66663049e-01\n",
      "  2.43623488e-01  2.48615406e-01  2.33964651e-01  1.17242637e-01\n",
      "  4.12707455e-02  2.34931367e-01  1.17291406e-01  2.09494015e-01\n",
      "  2.55653163e-01  7.31472676e-02  2.31024702e-01  3.25023739e-01\n",
      "  2.80363737e-01  1.70383293e-01  1.20194094e-01  3.28651117e-01\n",
      "  1.12358418e-01  2.64678111e-01  1.83450756e-01  9.05019057e-02\n",
      "  2.37785729e-02  1.67221985e-01  2.41367893e-01  2.35883140e-01\n",
      "  1.26394243e-01  1.54300807e-01  2.89407522e-01  2.70224493e-01\n",
      "  1.87076681e-01  2.10361252e-01  3.11838353e-01  3.28008013e-01\n",
      "  1.63268042e-01  2.25633283e-01  6.81951793e-02  2.42770285e-01\n",
      "  1.25179598e-01  1.37756291e-01  1.96091453e-01  2.29571544e-01\n",
      "  3.02712158e-01  1.92112066e-01  1.56462581e-01  1.22621221e-01\n",
      "  1.90899787e-01  1.92462428e-01  1.48038058e-01  2.80654376e-01\n",
      "  2.47809503e-01  1.52275560e-01  9.69592862e-02  2.21329405e-01\n",
      "  2.65405672e-01  4.66427350e-02  3.12415711e-01  3.02368743e-01\n",
      "  2.15315126e-01  8.71259795e-02  4.42233424e-01  1.74680869e-01\n",
      "  1.71967462e-01  2.19262352e-01  1.18589723e-01  1.73534031e-01\n",
      "  1.19139262e-01  2.50559587e-01  1.45039002e-01  2.26152966e-01\n",
      "  1.67027704e-01  1.89712614e-01  2.43029761e-01  3.30615037e-01\n",
      "  2.73241675e-01  2.77075992e-01  1.67440012e-01  1.89721539e-01\n",
      "  2.34635537e-01  3.55139805e-01  1.80226558e-01  1.34338232e-01\n",
      "  3.23844938e-01  1.99298181e-01  2.55660422e-01  9.03911127e-02\n",
      "  2.52361909e-01  2.77855397e-01  1.59795106e-01  1.32299909e-01\n",
      "  2.06542544e-01  3.28866470e-01  1.52168142e-01  2.39697644e-01\n",
      "  3.70007509e-01  1.49317420e-01  2.62045874e-01  2.44857869e-01\n",
      "  1.58155644e-01  2.72689460e-01  3.72971485e-01  3.63195305e-01\n",
      "  3.17569718e-01  2.24839487e-01  2.18505312e-01  2.06659573e-01\n",
      "  3.17504181e-01  1.89236749e-01  2.76148952e-01  1.97721036e-01\n",
      "  3.08009623e-01  2.35427900e-01  6.29022534e-01  3.27389442e-01\n",
      "  2.93508182e-01  4.97968987e-01  1.94272376e-01  3.33408065e-01\n",
      "  3.55231458e-01  2.75490665e-01  3.98936327e-01  4.95974553e-01\n",
      "  3.18222246e-01  2.34989951e-01  2.30308490e-01  2.29863548e-01\n",
      "  2.69891284e-01  3.44345065e-01  2.36814931e-01  3.49721916e-01\n",
      "  3.98703655e-01  2.42046232e-01  2.24531833e-01  2.44807110e-01\n",
      "  3.14534781e-01  1.02181914e-01  1.16615979e-01  1.61814150e-01\n",
      "  3.55339566e-01  3.11364950e-01  2.58969770e-01  2.45607899e-01\n",
      "  2.04426567e-01  2.65993659e-01  2.63307326e-01  2.39219453e-01\n",
      "  2.13131326e-01  2.41141668e-01  2.19448586e-01  2.85992072e-01\n",
      "  2.70326526e-01  2.61546837e-01  2.65700673e-01  2.18741753e-01\n",
      "  3.19220730e-01  3.29894342e-01  3.51827569e-01  3.29844047e-01\n",
      "  5.48886246e-01  2.69883865e-01  2.98846549e-01  2.83493314e-01\n",
      "  2.99903007e-01  3.14994025e-01  4.40412098e-01  2.27839428e-01\n",
      "  3.36364323e-01  5.59477483e-01  2.28678451e-01  4.30825113e-01\n",
      "  2.90383881e-01  2.27956768e-01  3.02709134e-01  1.61355009e-01\n",
      "  2.94401383e-01  2.79380832e-01  3.57262887e-01  2.50080796e-01\n",
      "  2.50773674e-01  3.03826758e-01  2.20083015e-01  4.45805206e-01\n",
      "  4.85524667e-01  2.02034057e-01  6.70387807e-01  5.05401495e-01\n",
      "  3.89652747e-01  3.19779566e-01  3.97427875e-01  5.23579064e-01\n",
      "  5.16622519e-01  4.82194304e-01  5.57947112e-01  3.84781110e-01\n",
      "  6.17756947e-01  5.04017769e-01  4.59316873e-01  1.96838676e-01\n",
      "  4.33886546e-01  4.58553813e-01  4.43215916e-01  7.10522921e-01\n",
      "  5.84861466e-01  2.78628483e-01  4.90618925e-01  3.50500187e-01\n",
      "  4.73959508e-01  6.22221615e-01  5.74396664e-01  5.99645001e-01\n",
      "  3.43656357e-01  5.93196824e-01  5.05401367e-01  4.82869426e-01\n",
      "  5.59336890e-01  4.86689635e-01  5.21347285e-01  3.05725575e-01\n",
      "  4.41326063e-01  4.36931561e-01  6.96038947e-01  6.79642482e-01\n",
      "  6.06820201e-01  6.86332927e-01  1.05870035e+00  9.89840261e-01\n",
      "  1.06029945e+00  9.13491997e-01  9.87128183e-01  9.72689487e-01\n",
      "  8.43280301e-01  1.06222770e+00  1.01607273e+00  1.14112049e+00\n",
      "  8.92205176e-01  1.14037961e+00  1.19717159e+00  1.13174255e+00\n",
      "  1.07027643e+00  1.20409112e+00  1.02482682e+00  1.08292284e+00\n",
      "  1.10428096e+00  1.09292368e+00  1.09105813e+00  1.00654926e+00\n",
      "  1.17586715e+00  1.11246508e+00  1.05928714e+00  1.15683494e+00\n",
      "  1.15631978e+00  1.11303187e+00  1.05856992e+00  1.13912826e+00\n",
      "  1.00231083e+00  1.10805403e+00  1.11146133e+00  1.15502936e+00]\n"
     ]
    }
   ],
   "source": [
    "print(kFoldRidge(k,X,z_n,0.))\n",
    "print(kFold(k,X,z_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLassoCV(k,X,z_n,_lambda):\n",
    "    partition = 0.2 #Percentage of data to use for testing\n",
    "\n",
    "    #How to select good tolerances and max iters?\n",
    "    reg = skl.LassoCV(alphas=_lambda,cv = k, random_state = 0,tol=0.0001,max_iter = 100000).fit(X,z_n)\n",
    "    ztilde = reg.predict(X)\n",
    "    plt.figure()\n",
    "    plt.semilogx(reg.alphas_,reg.mse_path_)\n",
    "    plt.semilogx(reg.alphas_,reg.mse_path_.mean(axis=-1),'k')\n",
    "    plt.xlabel('Log-plot of lambdas')\n",
    "    plt.ylabel('Mean squared error')\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_squared_error(ztilde,z_n), r2_score(ztilde,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLasso(k,X,z_n,_lambda):\n",
    "    # this converges slowly for small lambda!\n",
    "    \n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n]\n",
    "    np.random.shuffle(combi)\n",
    "    X, z_n = combi[:,:n], combi[:,n]\n",
    "    \n",
    "    #dont need to shuffle, as traintestsplit picks random?\n",
    "    \n",
    "    partition = 0.2 # percentage of data to use for testing\n",
    "    sMSE = 0 #sum mean squared error\n",
    "    sR2 = 0 #sum R2 score\n",
    "    \n",
    "    for i in range(k):\n",
    "        X_train, X_test, z_train, z_test = train_test_split(X,z_n,test_size=partition,shuffle=False )\n",
    "\n",
    "        lass = skl.Lasso(alpha=_lambda,tol = 0.0001,max_iter = 100000)\n",
    "        lass.fit(X_train,z_train)\n",
    "        z_tilde = lass.predict(X_test)\n",
    "\n",
    "#         # sum of MSE and R2, remember to divide by k later\n",
    "        sMSE += mean_squared_error(z_test,z_tilde)\n",
    "        sR2 += r2_score(z_test,z_tilde)\n",
    "\n",
    "        # Prepare data for next fold\n",
    "        X = np.roll(X,len(X_test),axis=0) # Rolls matrix downwards before next split\n",
    "        z_n = np.roll(z_n,len(z_test)) # Roll\n",
    "    \n",
    "    return sMSE/k, sR2/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7fc9c03c32bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMPD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     dMSE, dR2 = NoResampling(X,z_n,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     dMSE,dR2 = kFoldskl(k,X,z_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "MPD = np.arange(9)+1#np.arange(9)+1\n",
    "MSE_degree = []\n",
    "\n",
    "\n",
    "#shuffle data before doing the kFold\n",
    "n = len(X[0,:])\n",
    "combi = np.c_[X,z_n]\n",
    "np.random.shuffle(combi)\n",
    "X, z_n = combi[:,:n], combi[:,n]\n",
    "for i in MPD:\n",
    "    X = Model(x,y,i)\n",
    "    dMSE, dR2 = kFold(k,X,z_n)\n",
    "#     dMSE, dR2 = NoResampling(X,z_n,0)\n",
    "#     dMSE,dR2 = kFoldskl(k,X,z_n)\n",
    "    print(i,dMSE)\n",
    "    MSE_degree.append(dMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MPD+1,MSE_degree)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the model complexity isnt high enough for the curve to have a minimum like Hastie 2.11. To increase model complexity, a different inversion method is required, due to round-off-error, or what its called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = np.logspace(-4,5,10)\n",
    "\n",
    "RidgeMSE = np.zeros((len(MPD),len(_lambda)))\n",
    "RidgeR2 = np.zeros((len(MPD),len(_lambda)))\n",
    "\n",
    "\n",
    "\n",
    "for i,mpd in enumerate(MPD):\n",
    "    X = Model(x,y,mpd)\n",
    "    for j,lam in enumerate(_lambda):\n",
    "#         rMSE,rR2 = NoResampling(X,z_n,lam)\n",
    "        rMSE,rR2 = kFoldRidge(k,X,z_n,lam)\n",
    "        RidgeMSE[i][j] = rMSE\n",
    "        RidgeR2[i][j] = rR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(_lambda)):\n",
    "    plt.semilogy(MPD,RidgeMSE[:,i],label=_lambda[i])\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(MPD)):\n",
    "    plt.loglog(_lambda,RidgeMSE[i,:],label=i+1)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use Scikit-learn as recommended for this\n",
    "kFoldLassoCV(k,X,z_n,_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#morten spm:\n",
    "# confidence interval\n",
    "# bias variance: \n",
    "#use each fold to make mean, or run 10 shuffled k-folds?\n",
    "#lambdas are usually horrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
