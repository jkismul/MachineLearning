{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge svd?\n",
    "#wanna go QR _or_ cholesky or what its called? check monday\n",
    "#expand a bit to allow looping over noise weights?\n",
    "#first mse r2 for low N?\n",
    "#Most kFold versions look similar. introduce a \"method\" parameter and consolidate\n",
    "#bias also increases with model complexity?\n",
    "\n",
    "#validation set too?\n",
    "#CENTER DATA?\n",
    "#program likes cubic input, fix\n",
    "# Ridge, 0 lambda best?\n",
    "#VARIANCES SAME?!?!?!!?\n",
    "\n",
    "#split in 3, train test to find poly degree, then use thiese betas with validation set to adjust hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "from imageio import imread\n",
    "from random import random, seed\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "import scipy.linalg as scl\n",
    "from sklearn.model_selection import KFold\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an \"uglified\" version of the FrankeFunction, to minimize use of costly functions like divisions and powers.\n",
    "It takes as input parameters a meshgrid of coordinates in the x and y direction.\n",
    "\"\"\"\n",
    "def FrankeFunction(x,y): #still got one division in here\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)*(9*x-2)) - 0.25*((9*y-2)*(9*y-2)))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)*(9*x+1))/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)*(9*x-7)*0.25 - 0.25*((9*y-3)*(9*y-3)))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)*(9*x-4) - (9*y-7)*(9*y-7))\n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates the Model Matrix, usually dubbed X, for regression analysis.\n",
    "It takes as input parameters a meshgrid of coordinates in the x and y direction, \n",
    "and the polynomial degree P that you wish to fit.\n",
    "\n",
    "The order of columns is different from the one scikit learn creates, so take care to use the same model-creator when\n",
    "comparing scikit and this code.\n",
    "\n",
    "The ordering this function creates is: x^0y^0, x^1y^0, x^2y^0, x^0y^1, x^1y^1, x^0y^2 for a 2nd order polynomial.\n",
    "\"\"\"\n",
    "def Model(x,y,P): \n",
    "    m = len(x)*len(y) # number of equations\n",
    "    t = sum(range(P+2)) # number of terms in polynomial\n",
    "    X = np.zeros((m,t)) # Model matrix\n",
    "    a = np.matrix.flatten(x)\n",
    "    b = np.matrix.flatten(y)\n",
    "    c = 0 #counter\n",
    "    for i in range(P+1):\n",
    "        for j in range(P+1-i):\n",
    "            X[:,c] = a**j*b**i\n",
    "            c +=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_svd(X: np.ndarray, z: np.ndarray,_lambda) -> np.ndarray:\n",
    "    u, s, v = scl.svd(X)\n",
    "    return v.T @ scl.pinv(scl.diagsvd(s, u.shape[0], v.shape[0])) @ u.T @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_betas(X,z_n,_lambda):\n",
    "    return np.linalg.inv(X.T.dot(X)+_lambda*np.eye(len(X[0][:]))).dot(X.T).dot(z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just a standard R2 score calculator, taking the measured/real data as the first input, and the values the \n",
    "regression model finds as the second input\n",
    "\"\"\"\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard mean squared error calculator, inputs are measured/real data and regression model values.\n",
    "\"\"\"\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarOLS(y_data, y_model, X, sigma):\n",
    "    N = len(y_data)\n",
    "    covar = np.linalg.inv(X.T.dot(X))\n",
    "    vari = np.diagonal(covar)\n",
    "    return vari*(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarRidge(X,_lambda,sigma):\n",
    "    XX = X.T@X\n",
    "    invers = np.linalg.inv(XX+_lambda*np.eye(len(XX)))\n",
    "    return np.diagonal(invers)*(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoResampling(X,z_n,_lambda):\n",
    "    beta = np.linalg.inv(X.T.dot(X)+_lambda*np.eye(len(X[0][:]))).dot(X.T).dot(z_n)\n",
    "#     beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(z_n)\n",
    "#     beta = ols_svd(X,z_n,_lambda)\n",
    "#     beta = ols_inv(X,z_n)\n",
    "    ztilde = X @ beta\n",
    "    return MSE(z_n,ztilde), R2(z_n,ztilde), beta, ztilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bias(y_data, y_model):\n",
    "    n = np.size(y_data)\n",
    "    return np.sum((y_data-np.mean(y_model))**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shuffler(X,z_n):\n",
    "#     np.random.seed(1234)\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n] #combine\n",
    "    np.random.shuffle(combi) #shuffle\n",
    "    X, z_n = combi[:,:n], combi[:,n] #split\n",
    "    return X, z_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(X,z,k,_lambda):\n",
    "    #shuffle data\n",
    "    X,z_n = Shuffler(X,z)\n",
    "\n",
    "    #create splits\n",
    "    X_k = np.split(X, k)\n",
    "    z_k = np.split(z, k)\n",
    "\n",
    "    #initiate variables\n",
    "    MSE_test = []\n",
    "    Variance_test = []\n",
    "    bis = []\n",
    "    c=0\n",
    "    #perform kfold CV\n",
    "    for i in range(k):\n",
    "\n",
    "        X_train = X_k\n",
    "        z_train = z_k\n",
    "\n",
    "        X_test = X_k[i]\n",
    "        X_train = np.delete(X_train, i , 0)\n",
    "        X_train = np.concatenate(X_train)\n",
    "        z_test = z_k[i]\n",
    "        z_train = np.delete(z_train, i , 0)\n",
    "        z_train = np.ravel(z_train)\n",
    "\n",
    "        beta = Ridge_betas(X,z_n,0.)\n",
    "#         beta = ols_svd(X,z_n)\n",
    "        z_predict = X_test.dot(beta)\n",
    "        if c ==0:\n",
    "            print(MSE(z_test,z_predict))\n",
    "        c +=1\n",
    "        MSE_test = np.append(MSE_test, MSE(z_test, z_predict))\n",
    "        Variance_test = np.append(Variance_test,np.var(z_predict)) \n",
    "        bis = np.append(bis,Bias(z_test,z_predict))\n",
    "#     for i in range(len(z_predict)):\n",
    "#         print(z_predict[i]-z_test[i])\n",
    "#     print(np.mean(z_predict-z_test))\n",
    "# \n",
    "    return MSE_test, Variance_test,bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldskl(X,z,k,_lambda):\n",
    "    #shuffle data before doing the kFold\n",
    "    X, z_n = Shuffler(X,z)\n",
    "\n",
    "    #initiate variables\n",
    "    MSE_test = []\n",
    "    Variance_test = []\n",
    "    bis = []\n",
    "    \n",
    "    kfold = KFold(n_splits=k,shuffle=False) \n",
    "    kfold.get_n_splits(X)\n",
    "    c =0\n",
    "    \n",
    "    clfs = skl.LinearRegression(fit_intercept=False)\n",
    "    cv_res = cross_validate(clfs,X,z_n,cv=5)\n",
    "    \n",
    "    sorted(cv_res.keys())\n",
    "#     for train,test in kfold.split(X):\n",
    "#         # find parameters\n",
    "#         X_train, X_test = X[train],X[test]\n",
    "#         y_train,y_test = z_n[train],z_n[test]\n",
    "        \n",
    "#         clf = skl.LinearRegression(fit_intercept=False) #False to not center data, i.e. intercept is not 0\n",
    "#         clf.fit(X_train,y_train) \n",
    "\n",
    "#         #make prediction\n",
    "#         z_predict = clf.predict(X_test)\n",
    "        \n",
    "# #         MSE_test = np.append(MSE_test, MSE(z_n[test], z_predict))\n",
    "#         if c==0:\n",
    "#             print(MSE(y_test, z_predict))\n",
    "# #         Variance_test = np.append(Variance_test,np.var(z_predict)) \n",
    "# #         bis = np.append(bis,Bias(z_n[test],z_predict))\n",
    "#         c+=1\n",
    "#     print(z_predict)\n",
    "#     for i in range(len(z_predict)):\n",
    "#         print(z_predict[i]-z_n[test][i])\n",
    "#     print(np.mean(z_predict-z_n[test]))\n",
    "#     return MSE_test, Variance_test,bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRidge(X,z_n,k,_lambda):\n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "\n",
    "    #initialize values\n",
    "    sMSE = 0. #sum mean squared error\n",
    "    sR2 = 0. #sum R2 score\n",
    "    betas =[]\n",
    "    c=0\n",
    "    tilder = []\n",
    "    for X_train, X_test, z_train, z_test in kSplitter(X,z_n,k):\n",
    "        #center data for ridge\n",
    "        X_train = np.delete(X_train-np.mean(X_train,axis=0),0,1)\n",
    "        X_test = np.delete(X_test-np.mean(X_test,axis=0),0,1)\n",
    "        z_train = z_train -np.mean(z_train)\n",
    "        z_test = z_test-np.mean(z_test)\n",
    "        \n",
    "        \n",
    "        # find parameters\n",
    "        betas.append(np.linalg.inv(X_train.T.dot(X_train)+_lambda*np.eye(n-1)).dot(X_train.T).dot(z_train))\n",
    "        \n",
    "        # make prediction\n",
    "        z_tilde = X_test @ betas[c][:]\n",
    "        c+=1\n",
    "        tilder = np.hstack((tilder,z_tilde))\n",
    "\n",
    "        sMSE += MSE(z_tilde,z_test)\n",
    "        sR2 += R2(z_test,z_tilde)\n",
    "    \n",
    "    return sMSE/k,sR2/k,np.mean(betas,axis=0),tilder#,VAR SOMEHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLassoCV(k,X,z_n,_lambda):\n",
    "    _lambda = [_lambda]\n",
    "    #How to select good tolerances and max iters?\n",
    "    reg = skl.LassoCV(alphas=_lambda,cv = k, random_state = 0,tol=0.0001,max_iter = 10000000).fit(X,z_n)\n",
    "    ztilde = reg.predict(X)\n",
    "#     plt.figure()\n",
    "#     plt.semilogx(reg.alphas_,reg.mse_path_)\n",
    "#     plt.semilogx(reg.alphas_,reg.mse_path_.mean(axis=-1),'k')\n",
    "#     plt.xlabel('Log-plot of lambdas')\n",
    "#     plt.ylabel('Mean squared error')\n",
    "#     plt.show()\n",
    "  \n",
    "    return np.mean(reg.mse_path_) , r2_score(z_n,ztilde), reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLasso(k,X,z_n,_lambda):\n",
    "    # this converges slowly for small lambda!\n",
    "    \n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "    X,z_n = Shuffler(X,z_n)\n",
    "\n",
    "\n",
    "    #initialize values\n",
    "    sMSE = 0. #sum mean squared error\n",
    "    sR2 = 0. #sum R2 score\n",
    "    betas = []\n",
    "    \n",
    "    for X_train, X_test, z_train, z_test in kSplitter(X,z_n,k):\n",
    "        # find parameters\n",
    "        lass = skl.Lasso(alpha=_lambda,tol = 0.0001,max_iter=100000)\n",
    "        lass.fit(X_train,z_train)\n",
    "    \n",
    "        #make prediction\n",
    "        z_tilde = lass.predict(X_test)\n",
    "        betas.append(lass.coef_)\n",
    "        \n",
    "        sMSE+= MSE(z_tilde,z_n)\n",
    "        sR2 += R2(z_n,z_tilde)\n",
    "    \n",
    "    return sMSE/k,R2/k,np.mean(betas,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2952106796001067\n",
      "(80, 21)\n",
      "0.0009685499430440104\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.27113529052371366\n",
      "(80, 21)\n",
      "0.003326892380666976\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.2870320287623608\n",
      "(80, 21)\n",
      "0.0029573555336336772\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.31779650088546857\n",
      "(80, 21)\n",
      "0.001943616295929958\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.32680804135104324\n",
      "(80, 21)\n",
      "0.0013647508739076599\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.2992530256748226\n",
      "(80, 21)\n",
      "0.0011868548062486197\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.26747174221373954\n",
      "(80, 21)\n",
      "0.0013169151716443416\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.2618919479168279\n",
      "(80, 21)\n",
      "0.0011441608253134044\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.29624590802460843\n",
      "(80, 21)\n",
      "0.0027207256873365967\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "0.3104722287511355\n",
      "(80, 21)\n",
      "0.001509709091305057\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n",
      "(80, 21)\n"
     ]
    }
   ],
   "source": [
    "#Create random variables/predictors\n",
    "# np.random.seed(1234)\n",
    "N = 20 #number of points along x and y axes\n",
    "\n",
    "# sort this for meshgrid\n",
    "x = np.sort(np.random.uniform(0,1,N))\n",
    "y = np.sort(np.random.uniform(0,1,N))\n",
    "\n",
    "x, y = np.meshgrid(x,y,sparse=False)\n",
    "#create datapoints/results\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "# Create noise\n",
    "noise_weight = 0. #might wanna make a function for this to loop over weights?\n",
    "Noise = noise_weight*np.random.randn(N,N)\n",
    "\n",
    "#add noise\n",
    "z_noise = z+Noise\n",
    "\n",
    "#flatten for use in functions\n",
    "z_n = np.matrix.flatten(z_noise) ##### RAVEL?\n",
    "\n",
    "#number of folds for crossvalidation\n",
    "k=5\n",
    "\n",
    "X = Model(x,y,5)\n",
    "\n",
    "splits=5\n",
    "for i in range(10):\n",
    "    aii,bii,zii = kFold(X,z_n,5,0.)\n",
    "    au,bu,zu = kFoldskl(X,z_n,5,0.)\n",
    "#     print(np.mean(aii),np.mean(bii)+np.mean(zii))#,np.var(kFold(5,X,z_n)[4]))\n",
    "#     print(np.mean(au),np.mean(bu)+np.mean(zu))#,np.var(kFold(5,X,z_n)[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do stuff with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta values and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "\n",
    "with open('Oppgave_a.csv',mode='w') as Oppgave_a:\n",
    "    Oppgave_a = csv.writer(Oppgave_a, delimiter=',')\n",
    "    for i in poly_degrees:\n",
    "        X = Model(x,y,i)\n",
    "#         print(\"Order of polynomial =\",i)\n",
    "#         print(\"OLS\")\n",
    "        b_OLS = NoResampling(X,z_n,0.)[2]\n",
    "        z_OLS = NoResampling(X,z_n,0.)[3]\n",
    "        Oppgave_a.writerow(b_OLS)\n",
    "        Oppgave_a.writerow(VarOLS(z_n,z_til,X ,noise_weight))\n",
    "#         print(b_OLS)\n",
    "#         print(VarOLS(z_n,z_til,X ,noise_weight))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_mse = []\n",
    "poly_degrees = np.arange(1,9)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_mse.append(NoResampling(X,z_n,0.)[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_mse)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE for OLS, no resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_r2 = []\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_r2.append(NoResampling(X,z_n,0.)[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_r2)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.title(\"R2 for OLS, no resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta values and CI with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "\n",
    "print(\"kFold resampling with k = \", k)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    print(\"Order of polynomial =\",i)\n",
    "    print(\"OLS\")\n",
    "    \n",
    "    print(\"\\\\begin{table}[]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{{OLS method, polynomial degree {}}}\".format(i))\n",
    "    print(\"\\\\begin{tabular}{|c|cc|l|}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\multicolumn{1}{|l|}{\\\\multirow{2}{*}{Coefficient}} & \\\\multicolumn{2}{l|}{Confidence interval = $\\\\beta_{mean}$ $\\\\pm$ 2*$\\\\sigma_{mean}$} & \\\\multirow{2}{*}{$\\\\beta_{variance}$} \\\\\")\n",
    "    print(\"\\\\multicolumn{1}{|l|}{}\")   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b_OLS =kFold(k,X,z_n)[2]\n",
    "    z_OLS = kFold(k,X,z_n)[5]\n",
    "    print(\"\\\\begin{{table}}\")\n",
    "    print(\"Weight & Value & variance \\\\\")\n",
    "    LatexPrinter(b_OLS,z_OLS)\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\label{tab:no.1}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "#     print(b_OLS)\n",
    "#     print(VarOLS(z_n,z_til,X ,noise_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_k_mse = []\n",
    "poly_degrees = np.arange(1,9)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_k_mse.append(kFold(k,X,z_n)[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_mse)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE for OLS, kFold resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_k_r2 = []\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_k_r2.append(kFold(k,X,z_n)[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_k_r2)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.title(\"R2 for OLS, kFold resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias variance needs helpz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "lambda_range = np.logspace(-5,0,20)\n",
    "# print(\"kFold resampling with k = \", k)\n",
    "beta_path = np.zeros((len(poly_degrees),len(lambda_range),21))\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "#     print(\"Order of polynomial =\",i)\n",
    "#     print(\"OLS\")\n",
    "    for j,lam in enumerate(lambda_range):\n",
    "#         print(\"lambda value = \",j)\n",
    "        mid = kFoldRidge(k,X,z_n,lam)[2]\n",
    "#         for k in range(len(mid)):\n",
    "#             beta_path[i,j,k] = mid[k]\n",
    "#         beta_path[i-1,j] = kFoldRidge(k,X,z_n,lam)[2]\n",
    "#         beta_path[i-1,j].append(kFoldRidge(k,X,z_n,j)[2])\n",
    "#         z_ridge = kFoldRidge(k,X,z_n,j)[3]\n",
    "#         print(b_ridge)\n",
    "#         print(VarRidge(X ,j,noise_weight))\n",
    "# plt.figure()\n",
    "# plt.plot(beta_path[0,:])\n",
    "# plt.show()\n",
    "# print(np.shape(beta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kinda random stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpd = 8\n",
    "_lam = 0.0001\n",
    "No_resampling = []\n",
    "kfold_resampling = []\n",
    "kfold_resampling_skl = []\n",
    "kfold_ridge = []\n",
    "kfold_lasso = []\n",
    "\n",
    "for i in range(maxpd):\n",
    "    X = Model(x,y,i+1)\n",
    "    No_resampling.append(NoResampling(X,z_n,_lam)[0])\n",
    "    kfold_resampling.append(kFold(k,X,z_n)[0])\n",
    "    kfold_resampling_skl.append(kFoldskl(k,X,z_n)[0])\n",
    "    kfold_ridge.append(kFoldRidge(k,X,z_n,_lam)[0])\n",
    "#     kfold_lasso.append(kFoldLassoCV(k,X,z_n,0.000001)[0])\n",
    "plt.figure()\n",
    "plt.plot(No_resampling,'b')\n",
    "plt.plot(kfold_resampling,'r')\n",
    "plt.plot(kfold_resampling_skl,'y')\n",
    "plt.plot(kfold_ridge,'k')\n",
    "# plt.plot(kfold_lasso,'c')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd = 5\n",
    "\n",
    "_lams = np.logspace(-4,0,20)\n",
    "# _lams = np.linspace(1e-4,1.,20)\n",
    "\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "rish=[]\n",
    "\n",
    "for i in _lams:\n",
    "    fish.append(kFoldRidge(k,X,z_n,i)[0])\n",
    "    rish.append(kFoldRidge(k,X,z_n,i)[1])\n",
    "plt.figure()\n",
    "plt.plot(_lams,fish)\n",
    "plt.title(\"mse ridge\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(_lams,rish)\n",
    "plt.title(\"r2 ridge\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = np.delete(X-np.mean(X,axis=0),0,1)\n",
    "z_n_c = z_n -np.mean(z_n)\n",
    "\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "rish=[]\n",
    "\n",
    "for i in _lams:\n",
    "    fish.append(kFoldLassoCV(k,X,z_n,i)[0])\n",
    "    rish.append(kFoldLassoCV(k,X,z_n,i)[1])\n",
    "plt.figure()\n",
    "plt.plot(_lams,fish)\n",
    "plt.title(\"mse kfold lasso\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(_lams,rish)\n",
    "plt.title(\"r2 kfold lasso\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = 5\n",
    "mpd = 9\n",
    "k=10\n",
    "ms = []\n",
    "rs = []\n",
    "vs = []\n",
    "bs = []\n",
    "\n",
    "for i in range(mpd):\n",
    "    X = Model(x,y,i+1)\n",
    "    Xn,z_nn=Shuffler(X,z_n)\n",
    "    msi=0\n",
    "    rsi = 0\n",
    "    vsi = 0\n",
    "    bsi = 0\n",
    "    \n",
    "#     for l in range(NN):\n",
    "    msi = kFold(k,Xn,z_nn)[0]\n",
    "    rsi = kFold(k,Xn,z_nn)[1]\n",
    "    vsi = kFold(k,Xn,z_nn)[3]\n",
    "    bsi = kFold(k,Xn,z_nn)[4]\n",
    "    ms.append(msi)\n",
    "    rs.append(rsi)\n",
    "    vs.append(vsi)\n",
    "    bs.append(bsi)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(np.arange(mpd)+1,ms)\n",
    "plt.title(\"mse kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(vs)\n",
    "plt.title(\"Var kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "ulp = []\n",
    "for i in range(len(bs)):\n",
    "    ulp.append(bs[i])\n",
    "plt.figure()\n",
    "plt.plot(bs)\n",
    "# plt.plot(ms,'k')\n",
    "plt.title(\"Bias2 kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(mpd)+1,rs)\n",
    "# plt.title(\"r2 kFold for polydegree, 5 reruns\")\n",
    "# plt.show()\n",
    "\n",
    "# print(np.shape(bs),np.shape(vs))\n",
    "# plt.figure()\n",
    "# # plt.plot(np.arange(mpd)+1,ms)\n",
    "# # plt.plot(np.arange(mpd)+1,ms)\n",
    "# bst = [i /90 for i in bs]\n",
    "# plt.plot(bst)\n",
    "# plt.plot(vs)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run NN kFolds to create bias/variance plot\n",
    "#My bias increases again with model complexity\n",
    "NN=5\n",
    "mpd = 15\n",
    "qq = len(z_n)\n",
    "\n",
    "#sort noisy signal\n",
    "z_n_sorted = sorted(z_n)\n",
    "\n",
    "#initialize variables \n",
    "NMSE_k = np.zeros(shape=(NN,mpd)) #MSE stored for each run of kFold\n",
    "model_values = np.zeros(shape=(NN,mpd,qq))\n",
    "bias2 = np.zeros(mpd)\n",
    "var = np.zeros(mpd)\n",
    "bias2_ = np.zeros(shape=(mpd,qq))\n",
    "var_ = np.zeros(shape=(mpd,qq))\n",
    "\n",
    "# betas = np.zeros(shape=(NN,mpd))\n",
    "betas=[]\n",
    "#Run NN kFolds\n",
    "for l in range(NN):\n",
    "    #shuffle here\n",
    "    for i in range(mpd):\n",
    "        X = Model(x,y,i+1)\n",
    "#         model_values[l,i] = kFoldskl(k,X,z_n)[0]\n",
    "        model_values[l,i] = kFold(k,X,z_n)[0]\n",
    "        NMSE_k[l,i] = MSE(model_values[l,i],z_n_sorted)\n",
    "        if l == 0:\n",
    "            betas.append(kFold(k,X,z_n)[1])\n",
    "NMSE = np.mean(NMSE_k,axis=0)\n",
    "\n",
    "#Calculate bias and variance for each polynomial degree\n",
    "for i in range(mpd):\n",
    "    for m in range(qq):\n",
    "        bias2_[i,m] = np.mean(model_values[:,i,m])\n",
    "        var_[i,m] = np.var(model_values[:,i,m])\n",
    "    bias2[i]=np.mean((z_n_sorted-bias2_[i])**2)\n",
    "    var[i] = np.mean(var_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_noise = 8\n",
    "legs = 'MSE','Bias','var','Bias+Var'\n",
    "plt.figure()\n",
    "maxi = 8\n",
    "plt.plot(np.arange(1,maxi+1),NMSE[:maxi],'k')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi],'g')\n",
    "plt.plot(np.arange(1,maxi+1),var[:maxi],'b')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi]+var[:maxi],'ro')\n",
    "plt.xlabel('Ploynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE and bias as functions of polynomial degree, CV')\n",
    "plt.legend(legs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "xp = np.arange(0,1,0.05)\n",
    "yp = np.arange(0,1,0.05)\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "fz = FrankeFunction(xp,yp)\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1,projection='3d')\n",
    "    Xp = Model(xz,yz,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(20,20)\n",
    "    axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "#     axs.plot_surface(xp,yp,fz,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "    axs.set_zlabel('z')\n",
    "    axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "    axs.set_zlim(-0.10,1.40)\n",
    "    axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "xp = np.arange(0,1,0.05)\n",
    "yp = np.arange(0,1,0.05)\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "fz = FrankeFunction(xp,yp)\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1)\n",
    "    Xp = Model(xz,yz,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(20,20)\n",
    "#     plo = zp-fz\n",
    "    plo = np.subtract(zp,fz)\n",
    "#     print(\"a\",zp[0],\"b\",fz[0],\"c\",plo[0])\n",
    "    aa = axs.contourf(plo,cmap=cm.coolwarm)\n",
    "#     axs.plot(plo[0],plo[1])\n",
    "#     axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "#     axs.addaxes(xp)\n",
    "#     axs.set_zlabel('z')\n",
    "#     axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "#     axs.set_zlim(-0.10,1.40)\n",
    "#     axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "#     axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(aa,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_noise = 8\n",
    "legs = 'MSE','Bias','var','Bias+Var'\n",
    "plt.figure()\n",
    "maxi = 9\n",
    "plt.plot(np.arange(1,maxi+1),NMSE[:maxi],'k')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi],'g')\n",
    "plt.plot(np.arange(1,maxi+1),var[:maxi],'b')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi]+var[:maxi],'ro')\n",
    "plt.xlabel('Ploynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE and bias as functions of polynomial degree, CV')\n",
    "plt.legend(legs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxpd = 5\n",
    "for i in range(maxpd):\n",
    "    print(\"Polynomial degree\", i+1)\n",
    "    X = Model(x,y,i+1)\n",
    "#     X = X-np.mean(X,axis=0)\n",
    "#     X[:,0]=1.\n",
    "#     z_n = z_n -np.mean(z_n)\n",
    "    ztilde = kFold(k,X,z_n)[0]\n",
    "    print(\"OLS\")\n",
    "    print(\"beta\",kFold(k,X,z_n)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "    print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    ztilde = kFoldRidge(k,X,z_n,100.1)[0] #just a random lambda value\n",
    "    print(\"Ridge\")\n",
    "    print(\"beta\",kFoldRidge(k,X,z_n,0.1)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "#     print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    print(\"variance\",VarRidge(X,10000.1))\n",
    "    ztilde = kFoldLassoCV(k,X,z_n,[100.001,100.01])\n",
    "    print(\"Lasso\")\n",
    "    print(\"beta\",kFold(k,X,z_n)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "    print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    print(\"-------------------\")\n",
    "    ### ALL VARIANCES ARE THE SAME, WHY?!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lams = np.logspace(-4,0,10)\n",
    "kFoldLassoCV(k,X,z_n,_lams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPD = np.arange(9)+1#np.arange(9)+1\n",
    "MSE_degree = []\n",
    "\n",
    "\n",
    "#shuffle data before doing the kFold\n",
    "# n = len(X[0,:])\n",
    "# combi = np.c_[X,z_n]\n",
    "# np.random.shuffle(combi)\n",
    "# X, z_n = combi[:,:n], combi[:,n]\n",
    "# X_new,z_n_new = Shuffler(X,z_n)\n",
    "for i in MPD:\n",
    "    X = Model(x,y,i)\n",
    "    \n",
    "    z_out = kFold(k,X,z_n)\n",
    "#     dMSE, dR2 = NoResampling(X,z_n,0)\n",
    "#     dMSE,dR2 = kFoldskl(k,X,z_n)\n",
    "#     print(i,dMSE)\n",
    "    MSE_degree.append(MSE(z_out,z_n_sorted))\n",
    "# MS = MSE(MSE_degree,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MPD+1,MSE_degree)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = np.logspace(-4,5,10)\n",
    "\n",
    "RidgeMSE = np.zeros((len(MPD),len(_lambda)))\n",
    "RidgeR2 = np.zeros((len(MPD),len(_lambda)))\n",
    "\n",
    "\n",
    "\n",
    "for i,mpd in enumerate(MPD):\n",
    "    X = Model(x,y,mpd)\n",
    "    for j,lam in enumerate(_lambda):\n",
    "#         rMSE,rR2 = NoResampling(X,z_n,lam)\n",
    "        z_out = kFoldRidge(k,X,z_n,lam)\n",
    "        RidgeMSE[i][j] = MSE(z_out,z_n_sorted)\n",
    "        RidgeR2[i][j] = R2(z_n_sorted,z_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(_lambda)):\n",
    "    plt.semilogy(MPD,RidgeMSE[:,i],label=_lambda[i])\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(MPD)):\n",
    "    plt.loglog(_lambda,RidgeMSE[i,:],label=i+1)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use Scikit-learn as recommended for this\n",
    "kFoldLassoCV(k,X,z_n,_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain = imread('SRTM_data_Norway_1.tif')\n",
    "terr_square = terrain[0:100,0:100]\n",
    "print(np.shape(terr_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra = np.matrix.flatten(terr_square)\n",
    "print(np.shape(terra))\n",
    "# x = np.linspace(0,len(terr_square[0]),100)\n",
    "# y = np.linspace(0,len(terr_square[1]),100)\n",
    "x = np.linspace(0,1,np.sqrt(len(terra)))\n",
    "y=np.linspace(0,1,np.sqrt(len(terra)))\n",
    "x, y = np.meshgrid(x,y,sparse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Model(x,y,5)\n",
    "\n",
    "\n",
    "X_c = X-np.mean(X,axis=0)\n",
    "X_c[:,0]=1.\n",
    "terra_c = terra -np.mean(terra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoResampling(X,terra,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas=[]\n",
    "mpd = 6\n",
    "for i in range(mpd):\n",
    "        X = Model(xp,yp,i+1)\n",
    "        print(np.shape(X))\n",
    "#         model_values[l,i] = kFoldskl(k,X,z_n)[0]\n",
    "#         model_values[l,i] = kFold(k,X,z_n)[0]\n",
    "#         NMSE_k[l,i] = MSE(model_values[l,i],z_n_sorted)\n",
    "#         if l == 0:\n",
    "        betas.append(kFold(k,X,terra)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "# xp = np.arange(0,1,0.05)\n",
    "# yp = np.arange(0,1,0.05)\n",
    "\n",
    "\n",
    "xp = np.linspace(0,1,len(terr_square[0]))\n",
    "yp=np.linspace(0,1,len(terr_square[1]))\n",
    "\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "# fz = FrankeFunction(xp,yp)\n",
    "\n",
    "\n",
    "betas=[]\n",
    "for i in range(pd):\n",
    "        X = Model(xp,yp,i+1)\n",
    "        betas.append(kFold(k,X,terra)[1])\n",
    "\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1,projection='3d')\n",
    "    Xp = Model(xp,yp,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(len(xp),len(yp))\n",
    "    axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "#     axs.plot_surface(xp,yp,terr_square,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "    axs.set_zlabel('z')\n",
    "    axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "#     axs.set_zlim(-0.10,1.40)\n",
    "    axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "axers = fig2.gca(projection='3d')\n",
    "\n",
    "for i in range(pd):\n",
    "    surf = axers.plot_surface(xp,yp,terr_square,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = 5\n",
    "_lams = np.linspace(0.0,1.,10)\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "for i in range(len(_lams)):\n",
    "    fish.append(kFoldRidge(k,X,z_n,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Oppgave_a.csv\")\n",
    "# row2 = data.iloc[3]\n",
    "# print(row2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatexPrinter(betas,variances):\n",
    "    for i in range(len(betas)):\n",
    "        print(\"\\$\\beta_{{{}}}$ & {} & {} \\\\ \\hline\".format(i,betas[i],variances[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
