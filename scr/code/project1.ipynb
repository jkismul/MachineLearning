{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge svd?\n",
    "#wanna go QR _or_ cholesky or what its called? check monday\n",
    "#expand a bit to allow looping over noise weights?\n",
    "#first mse r2 for low N?\n",
    "#Most kFold versions look similar. introduce a \"method\" parameter and consolidate\n",
    "#bias also increases with model complexity?\n",
    "\n",
    "#validation set too?\n",
    "#CENTER DATA?\n",
    "#program likes cubic input, fix\n",
    "# Ridge, 0 lambda best?\n",
    "#VARIANCES SAME?!?!?!!?\n",
    "\n",
    "#split in 3, train test to find poly degree, then use thiese betas with validation set to adjust hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "from imageio import imread\n",
    "from random import random, seed\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "import scipy.linalg as scl\n",
    "from sklearn.model_selection import KFold\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an \"uglified\" version of the FrankeFunction, to minimize use of costly functions like divisions and powers.\n",
    "It takes as input parameters a meshgrid of coordinates in the x and y direction.\n",
    "\"\"\"\n",
    "def FrankeFunction(x,y): #still got one division in here\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)*(9*x-2)) - 0.25*((9*y-2)*(9*y-2)))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)*(9*x+1))/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)*(9*x-7)*0.25 - 0.25*((9*y-3)*(9*y-3)))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)*(9*x-4) - (9*y-7)*(9*y-7))\n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates the Model Matrix, usually dubbed X, for regression analysis.\n",
    "It takes as input parameters a meshgrid of coordinates in the x and y direction, \n",
    "and the polynomial degree P that you wish to fit.\n",
    "\n",
    "The order of columns is different from the one scikit learn creates, so take care to use the same model-creator when\n",
    "comparing scikit and this code.\n",
    "\n",
    "The ordering this function creates is: x^0y^0, x^1y^0, x^2y^0, x^0y^1, x^1y^1, x^0y^2 for a 2nd order polynomial.\n",
    "\"\"\"\n",
    "def Model(x,y,P): \n",
    "    m = len(x)*len(y) # number of equations\n",
    "    t = sum(range(P+2)) # number of terms in polynomial\n",
    "    X = np.zeros((m,t)) # Model matrix\n",
    "    a = np.matrix.flatten(x)\n",
    "    b = np.matrix.flatten(y)\n",
    "    c = 0 #counter\n",
    "    for i in range(P+1):\n",
    "        for j in range(P+1-i):\n",
    "            X[:,c] = a**j*b**i\n",
    "            c +=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_svd(X: np.ndarray, z: np.ndarray,_lambda) -> np.ndarray:\n",
    "    u, s, v = scl.svd(X)\n",
    "    return v.T @ scl.pinv(scl.diagsvd(s, u.shape[0], v.shape[0])) @ u.T @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_betas(X,z_n,_lambda):\n",
    "    return np.linalg.inv(X.T.dot(X)+_lambda*np.eye(len(X[0][:]))).dot(X.T).dot(z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just a standard R2 score calculator, taking the measured/real data as the first input, and the values the \n",
    "regression model finds as the second input\n",
    "\"\"\"\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard mean squared error calculator, inputs are measured/real data and regression model values.\n",
    "\"\"\"\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def VarOLS(y_data, y_model, X, sigma):\n",
    "def VarOLS(X, sigma):\n",
    "#     N = len(y_data)\n",
    "    covar = np.linalg.inv(X.T.dot(X))\n",
    "    vari = np.diagonal(covar)\n",
    "    return vari*(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarRidge(X,_lambda,sigma):\n",
    "    XX = X.T@X\n",
    "    invers = np.linalg.inv(XX+_lambda*np.eye(len(XX)))\n",
    "    return np.diagonal(invers)*(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoResampling(X,z_n,_lambda):\n",
    "    beta = np.linalg.inv(X.T.dot(X)+_lambda*np.eye(len(X[0][:]))).dot(X.T).dot(z_n)\n",
    "#     beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(z_n)\n",
    "#     beta = ols_svd(X,z_n,_lambda)\n",
    "#     beta = ols_inv(X,z_n)\n",
    "    ztilde = X @ beta\n",
    "    return MSE(z_n,ztilde), R2(z_n,ztilde), beta, ztilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bias(y_data, y_model):\n",
    "    n = np.size(y_data)\n",
    "    return np.sum((y_data-np.mean(y_model))**2)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shuffler(X,z_n):\n",
    "#     np.random.seed(1234)\n",
    "    n = len(X[0,:])\n",
    "    combi = np.c_[X,z_n] #combine\n",
    "    np.random.shuffle(combi) #shuffle\n",
    "    X, z_n = combi[:,:n], combi[:,n] #split\n",
    "    return X, z_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFold(X,z,k,_lambda):\n",
    "    #shuffle data\n",
    "    X,z_n = Shuffler(X,z)\n",
    "\n",
    "    #create splits\n",
    "    X_k = np.split(X, k)\n",
    "    z_k = np.split(z, k)\n",
    "\n",
    "    #initiate variables\n",
    "    MSE_test = []\n",
    "    Variance_test = []\n",
    "    bis = []\n",
    "    c=0\n",
    "    #perform kfold CV\n",
    "    varb =[]\n",
    "    tilder=[]\n",
    "    bep=[]\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        X_train = X_k\n",
    "        z_train = z_k\n",
    "\n",
    "        X_test = X_k[i]\n",
    "        X_train = np.delete(X_train, i , 0)\n",
    "        X_train = np.concatenate(X_train)\n",
    "        z_test = z_k[i]\n",
    "        z_train = np.delete(z_train, i , 0)\n",
    "        z_train = np.ravel(z_train)\n",
    "\n",
    "        beta = Ridge_betas(X,z_n,0.)\n",
    "#         beta = ols_svd(X,z_n)\n",
    "        z_predict = X_test.dot(beta)\n",
    "    \n",
    "        tilder = np.hstack((tilder,z_predict))\n",
    "\n",
    "#         if c ==0:\n",
    "#             print(MSE(z_test,z_predict))\n",
    "        c +=1\n",
    "#         clf = skl.LinearRegression(fit_intercept=False) #False to not center data, i.e. intercept is not 0\n",
    "#         clf.fit(X_train,z_train) \n",
    "\n",
    "        #make prediction\n",
    "#         z_predict = clf.predict(X_test)\n",
    "    \n",
    "        varb.append(VarOLS(X,0.1))\n",
    "        bep.append(beta)\n",
    "        MSE_test = np.append(MSE_test, MSE(z_test, z_predict))\n",
    "        Variance_test = np.append(Variance_test,np.var(z_predict)) \n",
    "        bis = np.append(bis,Bias(z_test,z_predict))\n",
    "\n",
    "#     for i in range(len(z_predict)):\n",
    "#         print(z_predict[i]-z_test[i])\n",
    "#     print(np.mean(z_predict-z_test))\n",
    "# \n",
    "#     print(beta)\n",
    "    return MSE_test, Variance_test,bis,varb,bep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.41363774e-02 3.80345862e+00 7.91227730e+01 3.44461814e+02\n",
      " 3.25088945e+02 4.36611595e+01 8.30324653e-01 1.93823399e+01\n",
      " 1.00179517e+02 1.22279698e+02 2.31742585e+01 1.67611529e+01\n",
      " 6.48418317e+01 7.43820037e+01 1.54693206e+01 8.68825425e+01\n",
      " 5.92773057e+01 1.59089403e+01 9.61875401e+01 1.05969116e+01\n",
      " 1.45595323e+01]\n"
     ]
    }
   ],
   "source": [
    "X = Model(x,y,5)\n",
    "print(np.mean(kFold(X,z_n,k,0.)[3],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldskl(X,z,k,_lambda):\n",
    "    #shuffle data before doing the kFold\n",
    "    X, z_n = Shuffler(X,z)\n",
    "\n",
    "    #initiate variables\n",
    "    MSE_test = []\n",
    "    Variance_test = []\n",
    "    bis = []\n",
    "    \n",
    "    kfold = KFold(n_splits=k,shuffle=False) \n",
    "\n",
    "    for train,test in kfold.split(X):\n",
    "        # find parameters\n",
    "        X_train, X_test = X[train],X[test]\n",
    "        z_train,z_test = z_n[train],z_n[test]\n",
    "        clf = skl.LinearRegression(fit_intercept=False) #False to not center data, i.e. intercept is not 0\n",
    "        clf.fit(X_train,z_train) \n",
    "        beta = clf.coef_\n",
    "#         z_predict = X_test@beta\n",
    "        #make prediction\n",
    "        z_predict = clf.predict(X_test)\n",
    "        MSE_test = np.append(MSE_test, MSE(z_test, z_predict))\n",
    "\n",
    "        Variance_test = np.append(Variance_test,np.var(z_predict)) \n",
    "        bis = np.append(bis,Bias(z_test,z_predict))\n",
    "#     print(beta)\n",
    "    return MSE_test, Variance_test,bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRidge(X,z_n,k,_lambda):\n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "\n",
    "    #initialize values\n",
    "    sMSE = 0. #sum mean squared error\n",
    "    sR2 = 0. #sum R2 score\n",
    "    betas =[]\n",
    "    c=0\n",
    "    tilder = []\n",
    "    for X_train, X_test, z_train, z_test in kSplitter(X,z_n,k):\n",
    "        #center data for ridge\n",
    "        X_train = np.delete(X_train-np.mean(X_train,axis=0),0,1)\n",
    "        X_test = np.delete(X_test-np.mean(X_test,axis=0),0,1)\n",
    "        z_train = z_train -np.mean(z_train)\n",
    "        z_test = z_test-np.mean(z_test)\n",
    "        \n",
    "        \n",
    "        # find parameters\n",
    "        betas.append(np.linalg.inv(X_train.T.dot(X_train)+_lambda*np.eye(n-1)).dot(X_train.T).dot(z_train))\n",
    "        \n",
    "        # make prediction\n",
    "        z_tilde = X_test @ betas[c][:]\n",
    "        c+=1\n",
    "        tilder = np.hstack((tilder,z_tilde))\n",
    "\n",
    "        sMSE += MSE(z_tilde,z_test)\n",
    "        sR2 += R2(z_test,z_tilde)\n",
    "    \n",
    "    return sMSE/k,sR2/k,np.mean(betas,axis=0),tilder#,VAR SOMEHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLassoCV(k,X,z_n,_lambda):\n",
    "    _lambda = [_lambda]\n",
    "    #How to select good tolerances and max iters?\n",
    "    reg = skl.LassoCV(alphas=_lambda,cv = k, random_state = 0,tol=0.0001,max_iter = 10000000).fit(X,z_n)\n",
    "    ztilde = reg.predict(X)\n",
    "#     plt.figure()\n",
    "#     plt.semilogx(reg.alphas_,reg.mse_path_)\n",
    "#     plt.semilogx(reg.alphas_,reg.mse_path_.mean(axis=-1),'k')\n",
    "#     plt.xlabel('Log-plot of lambdas')\n",
    "#     plt.ylabel('Mean squared error')\n",
    "#     plt.show()\n",
    "  \n",
    "    return np.mean(reg.mse_path_) , r2_score(z_n,ztilde), reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldLasso(k,X,z_n,_lambda):\n",
    "    # this converges slowly for small lambda!\n",
    "    \n",
    "    #shuffle data before doing the kFold\n",
    "    n = len(X[0,:])\n",
    "    X,z_n = Shuffler(X,z_n)\n",
    "\n",
    "\n",
    "    #initialize values\n",
    "    sMSE = 0. #sum mean squared error\n",
    "    sR2 = 0. #sum R2 score\n",
    "    betas = []\n",
    "    \n",
    "    for X_train, X_test, z_train, z_test in kSplitter(X,z_n,k):\n",
    "        # find parameters\n",
    "        lass = skl.Lasso(alpha=_lambda,tol = 0.0001,max_iter=100000)\n",
    "        lass.fit(X_train,z_train)\n",
    "    \n",
    "        #make prediction\n",
    "        z_tilde = lass.predict(X_test)\n",
    "        betas.append(lass.coef_)\n",
    "        \n",
    "        sMSE+= MSE(z_tilde,z_n)\n",
    "        sR2 += R2(z_n,z_tilde)\n",
    "    \n",
    "    return sMSE/k,R2/k,np.mean(betas,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.27171554, 0.33065717, 0.22678097, 0.26341398, 0.29521604]),\n",
       " array([0.11577503, 0.13546978, 0.12087957, 0.12988475, 0.12645605]),\n",
       " array([0.15449372, 0.16072806, 0.11710132, 0.13468889, 0.17270874]),\n",
       " [array([2.88553886e-03, 5.26515488e-01, 2.24654510e+01, 1.87719726e+02,\n",
       "         3.14333153e+02, 7.09030984e+01, 6.15230233e-01, 1.07690156e+01,\n",
       "         6.94180922e+01, 1.12605668e+02, 3.16299889e+01, 1.95242871e+01,\n",
       "         6.06800404e+01, 7.78399700e+01, 2.46659518e+01, 1.15274933e+02,\n",
       "         8.17911146e+01, 1.83355255e+01, 1.34494661e+02, 1.70931941e+01,\n",
       "         2.09367716e+01]),\n",
       "  array([2.88553886e-03, 5.26515488e-01, 2.24654510e+01, 1.87719726e+02,\n",
       "         3.14333153e+02, 7.09030984e+01, 6.15230233e-01, 1.07690156e+01,\n",
       "         6.94180922e+01, 1.12605668e+02, 3.16299889e+01, 1.95242871e+01,\n",
       "         6.06800404e+01, 7.78399700e+01, 2.46659518e+01, 1.15274933e+02,\n",
       "         8.17911146e+01, 1.83355255e+01, 1.34494661e+02, 1.70931941e+01,\n",
       "         2.09367716e+01]),\n",
       "  array([2.88553886e-03, 5.26515488e-01, 2.24654510e+01, 1.87719726e+02,\n",
       "         3.14333153e+02, 7.09030984e+01, 6.15230233e-01, 1.07690156e+01,\n",
       "         6.94180922e+01, 1.12605668e+02, 3.16299889e+01, 1.95242871e+01,\n",
       "         6.06800404e+01, 7.78399700e+01, 2.46659518e+01, 1.15274933e+02,\n",
       "         8.17911146e+01, 1.83355255e+01, 1.34494661e+02, 1.70931941e+01,\n",
       "         2.09367716e+01]),\n",
       "  array([2.88553886e-03, 5.26515488e-01, 2.24654510e+01, 1.87719726e+02,\n",
       "         3.14333153e+02, 7.09030984e+01, 6.15230233e-01, 1.07690156e+01,\n",
       "         6.94180922e+01, 1.12605668e+02, 3.16299889e+01, 1.95242871e+01,\n",
       "         6.06800404e+01, 7.78399700e+01, 2.46659518e+01, 1.15274933e+02,\n",
       "         8.17911146e+01, 1.83355255e+01, 1.34494661e+02, 1.70931941e+01,\n",
       "         2.09367716e+01]),\n",
       "  array([2.88553886e-03, 5.26515488e-01, 2.24654510e+01, 1.87719726e+02,\n",
       "         3.14333153e+02, 7.09030984e+01, 6.15230233e-01, 1.07690156e+01,\n",
       "         6.94180922e+01, 1.12605668e+02, 3.16299889e+01, 1.95242871e+01,\n",
       "         6.06800404e+01, 7.78399700e+01, 2.46659518e+01, 1.15274933e+02,\n",
       "         8.17911146e+01, 1.83355255e+01, 1.34494661e+02, 1.70931941e+01,\n",
       "         2.09367716e+01])],\n",
       " [array([ 6.09405258e-01,  2.65377221e+00,  3.19270128e-02, -4.73874412e+01,\n",
       "          9.61896601e+01, -5.38619401e+01,  2.93251013e+00,  1.81600352e+00,\n",
       "         -7.93159363e+00, -2.03355364e+00,  6.42287108e+00, -9.01449902e+00,\n",
       "          1.57929207e+00,  4.03562372e+01, -1.99139106e+01, -4.08331119e+00,\n",
       "         -2.66206758e+01, -1.23270754e+01,  2.45692511e+01,  1.80682221e+01,\n",
       "         -1.47282835e+01]),\n",
       "  array([ 6.09405258e-01,  2.65377221e+00,  3.19270128e-02, -4.73874412e+01,\n",
       "          9.61896601e+01, -5.38619401e+01,  2.93251013e+00,  1.81600352e+00,\n",
       "         -7.93159363e+00, -2.03355364e+00,  6.42287108e+00, -9.01449902e+00,\n",
       "          1.57929207e+00,  4.03562372e+01, -1.99139106e+01, -4.08331119e+00,\n",
       "         -2.66206758e+01, -1.23270754e+01,  2.45692511e+01,  1.80682221e+01,\n",
       "         -1.47282835e+01]),\n",
       "  array([ 6.09405258e-01,  2.65377221e+00,  3.19270128e-02, -4.73874412e+01,\n",
       "          9.61896601e+01, -5.38619401e+01,  2.93251013e+00,  1.81600352e+00,\n",
       "         -7.93159363e+00, -2.03355364e+00,  6.42287108e+00, -9.01449902e+00,\n",
       "          1.57929207e+00,  4.03562372e+01, -1.99139106e+01, -4.08331119e+00,\n",
       "         -2.66206758e+01, -1.23270754e+01,  2.45692511e+01,  1.80682221e+01,\n",
       "         -1.47282835e+01]),\n",
       "  array([ 6.09405258e-01,  2.65377221e+00,  3.19270128e-02, -4.73874412e+01,\n",
       "          9.61896601e+01, -5.38619401e+01,  2.93251013e+00,  1.81600352e+00,\n",
       "         -7.93159363e+00, -2.03355364e+00,  6.42287108e+00, -9.01449902e+00,\n",
       "          1.57929207e+00,  4.03562372e+01, -1.99139106e+01, -4.08331119e+00,\n",
       "         -2.66206758e+01, -1.23270754e+01,  2.45692511e+01,  1.80682221e+01,\n",
       "         -1.47282835e+01]),\n",
       "  array([ 6.09405258e-01,  2.65377221e+00,  3.19270128e-02, -4.73874412e+01,\n",
       "          9.61896601e+01, -5.38619401e+01,  2.93251013e+00,  1.81600352e+00,\n",
       "         -7.93159363e+00, -2.03355364e+00,  6.42287108e+00, -9.01449902e+00,\n",
       "          1.57929207e+00,  4.03562372e+01, -1.99139106e+01, -4.08331119e+00,\n",
       "         -2.66206758e+01, -1.23270754e+01,  2.45692511e+01,  1.80682221e+01,\n",
       "         -1.47282835e+01])])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create random variables/predictors\n",
    "# np.random.seed(1234)\n",
    "N = 20 #number of points along x and y axes\n",
    "\n",
    "# sort this for meshgrid\n",
    "x = np.sort(np.random.uniform(0,1,N))\n",
    "y = np.sort(np.random.uniform(0,1,N))\n",
    "\n",
    "x, y = np.meshgrid(x,y,sparse=False)\n",
    "#create datapoints/results\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "# Create noise\n",
    "noise_weight = 0.1 #might wanna make a function for this to loop over weights?\n",
    "Noise = noise_weight*np.random.randn(N,N)\n",
    "\n",
    "#add noise\n",
    "z_noise = z+Noise\n",
    "\n",
    "#flatten for use in functions\n",
    "z_n = np.matrix.flatten(z_noise) ##### RAVEL?\n",
    "\n",
    "#number of folds for crossvalidation\n",
    "k=5\n",
    "\n",
    "X = Model(x,y,5)\n",
    "from sklearn.model_selection import cross_validate\n",
    "splits=5\n",
    "# kFoldskl(X,z_n,5,0.)\n",
    "kFold(X,z_n,5,0.)\n",
    "# for i in range(10):\n",
    "#     aii,bii,zii = kFold(X,z_n,5,0.)\n",
    "#     au,bu,zu = kFoldskl(X,z_n,5,0.)\n",
    "#     print(np.mean(aii),np.mean(bii)+np.mean(zii))#,np.var(kFold(5,X,z_n)[4]))\n",
    "#     print(np.mean(au),np.mean(bu)+np.mean(zu))#,np.var(kFold(5,X,z_n)[4]))\n",
    "\n",
    "\n",
    "# The problem with scikit learn was again how it splits data.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do stuff with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta values and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "\n",
    "with open('Oppgave_a.csv',mode='w') as Oppgave_a:\n",
    "    Oppgave_a = csv.writer(Oppgave_a, delimiter=',')\n",
    "    for i in poly_degrees:\n",
    "        X = Model(x,y,i)\n",
    "#         print(\"Order of polynomial =\",i)\n",
    "#         print(\"OLS\")\n",
    "        b_OLS = NoResampling(X,z_n,0.)[2]\n",
    "        z_OLS = NoResampling(X,z_n,0.)[3]\n",
    "        Oppgave_a.writerow(b_OLS)\n",
    "        Oppgave_a.writerow(VarOLS(z_n,z_til,X ,noise_weight))\n",
    "#         print(b_OLS)\n",
    "#         print(VarOLS(z_n,z_til,X ,noise_weight))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_mse = []\n",
    "poly_degrees = np.arange(1,9)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_mse.append(NoResampling(X,z_n,0.)[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_mse)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE for OLS, no resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_r2 = []\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_r2.append(NoResampling(X,z_n,0.)[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_r2)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.title(\"R2 for OLS, no resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta values and CI with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kFold resampling with k =  5\n",
      "Order of polynomial = 1\n",
      "OLS\n",
      "\\begin{table}[]\n",
      "\\centering\n",
      "\\caption{OLS method, polynomial degree 1}\n",
      "\\begin{tabular}{|c|cc|l|}\n",
      "\\hline\n",
      "\\multicolumn{1}{|l|}{\\multirow{2}{*}{Coefficient}} & \\multicolumn{2}{l|}{Confidence interval = $\\beta_{mean}$ $\\pm$ 2*$\\sigma_{mean}$} & \\multirow{2}{*}{$\\beta_{variance}$} \\\\\n",
      "\\multicolumn{1}{|l|}{}  & $\\beta_{mean}$                        & $\\sigma_{mean}$                       &                                     \\\\ \\hline\n",
      "$\\beta_{0}$ & 1.1288115531435463 & 0.00011184254828260094 \\\\ \n",
      "$\\beta_{1}$ & -0.5684381523621908 & 0.0003678604571736909 \\\\ \n",
      "$\\beta_{1}$ & -0.5684381523621908 & 0.0003678604571736909 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\label{tab:no.1}\n",
      "\\end{table}\n",
      "Order of polynomial = 2\n",
      "OLS\n",
      "\\begin{table}[]\n",
      "\\centering\n",
      "\\caption{OLS method, polynomial degree 2}\n",
      "\\begin{tabular}{|c|cc|l|}\n",
      "\\hline\n",
      "\\multicolumn{1}{|l|}{\\multirow{2}{*}{Coefficient}} & \\multicolumn{2}{l|}{Confidence interval = $\\beta_{mean}$ $\\pm$ 2*$\\sigma_{mean}$} & \\multirow{2}{*}{$\\beta_{variance}$} \\\\\n",
      "\\multicolumn{1}{|l|}{}  & $\\beta_{mean}$                        & $\\sigma_{mean}$                       &                                     \\\\ \\hline\n",
      "$\\beta_{0}$ & 1.1047184242613195 & 0.0003585433705348229 \\\\ \n",
      "$\\beta_{1}$ & -0.3349725139699228 & 0.006320420033094393 \\\\ \n",
      "$\\beta_{2}$ & -0.6768175724370711 & 0.007832586985579091 \\\\ \n",
      "$\\beta_{3}$ & -0.5953930054666015 & 0.005379454878594976 \\\\ \n",
      "$\\beta_{4}$ & 0.6799552924507914 & 0.0032592487147428116 \\\\ \n",
      "$\\beta_{4}$ & 0.6799552924507914 & 0.0032592487147428116 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\label{tab:no.2}\n",
      "\\end{table}\n",
      "Order of polynomial = 3\n",
      "OLS\n",
      "\\begin{table}[]\n",
      "\\centering\n",
      "\\caption{OLS method, polynomial degree 3}\n",
      "\\begin{tabular}{|c|cc|l|}\n",
      "\\hline\n",
      "\\multicolumn{1}{|l|}{\\multirow{2}{*}{Coefficient}} & \\multicolumn{2}{l|}{Confidence interval = $\\beta_{mean}$ $\\pm$ 2*$\\sigma_{mean}$} & \\multirow{2}{*}{$\\beta_{variance}$} \\\\\n",
      "\\multicolumn{1}{|l|}{}  & $\\beta_{mean}$                        & $\\sigma_{mean}$                       &                                     \\\\ \\hline\n",
      "$\\beta_{0}$ & 0.7532226509062481 & 0.0008325188705759635 \\\\ \n",
      "$\\beta_{1}$ & 2.1661774505252707 & 0.03588114149949184 \\\\ \n",
      "$\\beta_{2}$ & -8.14720800443935 & 0.22271926572183326 \\\\ \n",
      "$\\beta_{3}$ & 5.430062850549758 & 0.13830207645083661 \\\\ \n",
      "$\\beta_{4}$ & 1.9042525752846866 & 0.03269350233696408 \\\\ \n",
      "$\\beta_{5}$ & 0.4953817408288225 & 0.12057365047350824 \\\\ \n",
      "$\\beta_{6}$ & 2.0161161164439547 & 0.06939682852024258 \\\\ \n",
      "$\\beta_{7}$ & -7.18195578470723 & 0.14161283507042466 \\\\ \n",
      "$\\beta_{8}$ & -1.406952371797096 & 0.06560731796981338 \\\\ \n",
      "$\\beta_{8}$ & -1.406952371797096 & 0.06560731796981338 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\label{tab:no.3}\n",
      "\\end{table}\n",
      "Order of polynomial = 4\n",
      "OLS\n",
      "\\begin{table}[]\n",
      "\\centering\n",
      "\\caption{OLS method, polynomial degree 4}\n",
      "\\begin{tabular}{|c|cc|l|}\n",
      "\\hline\n",
      "\\multicolumn{1}{|l|}{\\multirow{2}{*}{Coefficient}} & \\multicolumn{2}{l|}{Confidence interval = $\\beta_{mean}$ $\\pm$ 2*$\\sigma_{mean}$} & \\multirow{2}{*}{$\\beta_{variance}$} \\\\\n",
      "\\multicolumn{1}{|l|}{}  & $\\beta_{mean}$                        & $\\sigma_{mean}$                       &                                     \\\\ \\hline\n",
      "$\\beta_{0}$ & 0.4802495623544031 & 0.0016314851208894866 \\\\ \n",
      "$\\beta_{1}$ & 6.087945876706535 & 0.15487443576401969 \\\\ \n",
      "$\\beta_{2}$ & -25.480457306683736 & 3.0728010518478506 \\\\ \n",
      "$\\beta_{3}$ & 31.27225795263059 & 9.969280781637782 \\\\ \n",
      "$\\beta_{4}$ & -12.27698909024554 & 3.5699706257796513 \\\\ \n",
      "$\\beta_{5}$ & 4.144275738044955 & 0.15526881216317004 \\\\ \n",
      "$\\beta_{6}$ & -5.830290297272189 & 1.572835997802115 \\\\ \n",
      "$\\beta_{7}$ & 18.75320554443902 & 3.3482369654806936 \\\\ \n",
      "$\\beta_{8}$ & -12.233046858118826 & 1.2253583013052634 \\\\ \n",
      "$\\beta_{9}$ & -15.875108412051699 & 2.2067776703557143 \\\\ \n",
      "$\\beta_{10}$ & 0.9114621205674946 & 2.9174796022217824 \\\\ \n",
      "$\\beta_{11}$ & -1.9645091933321823 & 1.396929229192452 \\\\ \n",
      "$\\beta_{12}$ & 18.125708581979453 & 4.814864142388854 \\\\ \n",
      "$\\beta_{13}$ & -0.4775332516537385 & 0.8611349989291206 \\\\ \n",
      "$\\beta_{13}$ & -0.4775332516537385 & 0.8611349989291206 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\label{tab:no.4}\n",
      "\\end{table}\n",
      "Order of polynomial = 5\n",
      "OLS\n",
      "\\begin{table}[]\n",
      "\\centering\n",
      "\\caption{OLS method, polynomial degree 5}\n",
      "\\begin{tabular}{|c|cc|l|}\n",
      "\\hline\n",
      "\\multicolumn{1}{|l|}{\\multirow{2}{*}{Coefficient}} & \\multicolumn{2}{l|}{Confidence interval = $\\beta_{mean}$ $\\pm$ 2*$\\sigma_{mean}$} & \\multirow{2}{*}{$\\beta_{variance}$} \\\\\n",
      "\\multicolumn{1}{|l|}{}  & $\\beta_{mean}$                        & $\\sigma_{mean}$                       &                                     \\\\ \\hline\n",
      "$\\beta_{0}$ & 0.6094052579778759 & 0.0028855388648592263 \\\\ \n",
      "$\\beta_{1}$ & 2.653772204137693 & 0.5265154882007476 \\\\ \n",
      "$\\beta_{2}$ & 0.03192703121250062 & 22.46545104731237 \\\\ \n",
      "$\\beta_{3}$ & -47.38744120824229 & 187.7197261307648 \\\\ \n",
      "$\\beta_{4}$ & 96.18966004774094 & 314.3331527831964 \\\\ \n",
      "$\\beta_{5}$ & -53.861940032266446 & 70.90309835653682 \\\\ \n",
      "$\\beta_{6}$ & 2.9325101240929707 & 0.6152302343625841 \\\\ \n",
      "$\\beta_{7}$ & 1.8160035280390332 & 10.76901563302111 \\\\ \n",
      "$\\beta_{8}$ & -7.931593683230965 & 69.41809219849144 \\\\ \n",
      "$\\beta_{9}$ & -2.033553528098409 & 112.60566837067219 \\\\ \n",
      "$\\beta_{10}$ & 6.422871014005298 & 31.62998889145822 \\\\ \n",
      "$\\beta_{11}$ & -9.01449900886744 & 19.524287161690697 \\\\ \n",
      "$\\beta_{12}$ & 1.579292061611934 & 60.680040420831574 \\\\ \n",
      "$\\beta_{13}$ & 40.356237155418796 & 77.83997004559433 \\\\ \n",
      "$\\beta_{14}$ & -19.913910631805113 & 24.6659518010898 \\\\ \n",
      "$\\beta_{15}$ & -4.08331121511387 & 115.27493315531356 \\\\ \n",
      "$\\beta_{16}$ & -26.620675765218557 & 81.79111458121992 \\\\ \n",
      "$\\beta_{17}$ & -12.327075373928007 & 18.335525481577946 \\\\ \n",
      "$\\beta_{18}$ & 24.56925113324005 & 134.4946614254769 \\\\ \n",
      "$\\beta_{19}$ & 18.06822212011656 & 17.093194130740073 \\\\ \n",
      "$\\beta_{19}$ & 18.06822212011656 & 17.093194130740073 \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\label{tab:no.5}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "\n",
    "print(\"kFold resampling with k = \", k)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    print(\"Order of polynomial =\",i)\n",
    "    print(\"OLS\")\n",
    "    \n",
    "    print(\"\\\\begin{table}[]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{{OLS method, polynomial degree {}}}\".format(i))\n",
    "    print(\"\\\\begin{tabular}{|c|cc|l|}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\multicolumn{1}{|l|}{\\\\multirow{2}{*}{Coefficient}} & \\\\multicolumn{2}{l|}{Confidence interval = $\\\\beta_{mean}$ $\\\\pm$ 2*$\\\\sigma_{mean}$} & \\\\multirow{2}{*}{$\\\\beta_{variance}$} \\\\\\\\\")\n",
    "    print(\"\\\\multicolumn{1}{|l|}{}  & $\\\\beta_{mean}$                        & $\\\\sigma_{mean}$                       &                                     \\\\\\\\ \\hline\")   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b_OLS =np.mean(kFold(X,z_n,k,0.)[4],axis=0)\n",
    "    z_OLS =np.mean(kFold(X,z_n,k,0.)[3],axis=0)\n",
    "    \n",
    "#     print(np.shape(b_OLS),np.shape(z_OLS))\n",
    "#     print(\"\\\\begin{{table}}\")\n",
    "#     print(\"Weight & Value & variance \\\\\")\n",
    "    LatexPrinter(b_OLS,z_OLS)\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\label{{tab:no.{}}}\".format(i))\n",
    "    print(\"\\\\end{table}\")\n",
    "    \n",
    "#         print(\"$\\\\beta_{{{}}}$ & {} & {} \\\\\\ \\\\hline\".format(i,betas[i],variances[i]))\n",
    "#     print(b_OLS)\n",
    "#     print(VarOLS(z_n,z_tilde,X ,noise_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_k_mse = []\n",
    "poly_degrees = np.arange(1,9)\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_k_mse.append(kFold(k,X,z_n)[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_mse)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE for OLS, kFold resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_k_r2 = []\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "    ols_k_r2.append(kFold(k,X,z_n)[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(poly_degrees,ols_k_r2)\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.title(\"R2 for OLS, kFold resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias variance needs helpz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degrees=np.arange(1,6)\n",
    "lambda_range = np.logspace(-5,0,20)\n",
    "# print(\"kFold resampling with k = \", k)\n",
    "beta_path = np.zeros((len(poly_degrees),len(lambda_range),21))\n",
    "for i in poly_degrees:\n",
    "    X = Model(x,y,i)\n",
    "#     print(\"Order of polynomial =\",i)\n",
    "#     print(\"OLS\")\n",
    "    for j,lam in enumerate(lambda_range):\n",
    "#         print(\"lambda value = \",j)\n",
    "        mid = kFoldRidge(k,X,z_n,lam)[2]\n",
    "#         for k in range(len(mid)):\n",
    "#             beta_path[i,j,k] = mid[k]\n",
    "#         beta_path[i-1,j] = kFoldRidge(k,X,z_n,lam)[2]\n",
    "#         beta_path[i-1,j].append(kFoldRidge(k,X,z_n,j)[2])\n",
    "#         z_ridge = kFoldRidge(k,X,z_n,j)[3]\n",
    "#         print(b_ridge)\n",
    "#         print(VarRidge(X ,j,noise_weight))\n",
    "# plt.figure()\n",
    "# plt.plot(beta_path[0,:])\n",
    "# plt.show()\n",
    "# print(np.shape(beta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kinda random stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpd = 8\n",
    "_lam = 0.0001\n",
    "No_resampling = []\n",
    "kfold_resampling = []\n",
    "kfold_resampling_skl = []\n",
    "kfold_ridge = []\n",
    "kfold_lasso = []\n",
    "\n",
    "for i in range(maxpd):\n",
    "    X = Model(x,y,i+1)\n",
    "    No_resampling.append(NoResampling(X,z_n,_lam)[0])\n",
    "    kfold_resampling.append(kFold(k,X,z_n)[0])\n",
    "    kfold_resampling_skl.append(kFoldskl(k,X,z_n)[0])\n",
    "    kfold_ridge.append(kFoldRidge(k,X,z_n,_lam)[0])\n",
    "#     kfold_lasso.append(kFoldLassoCV(k,X,z_n,0.000001)[0])\n",
    "plt.figure()\n",
    "plt.plot(No_resampling,'b')\n",
    "plt.plot(kfold_resampling,'r')\n",
    "plt.plot(kfold_resampling_skl,'y')\n",
    "plt.plot(kfold_ridge,'k')\n",
    "# plt.plot(kfold_lasso,'c')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd = 5\n",
    "\n",
    "_lams = np.logspace(-4,0,20)\n",
    "# _lams = np.linspace(1e-4,1.,20)\n",
    "\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "rish=[]\n",
    "\n",
    "for i in _lams:\n",
    "    fish.append(kFoldRidge(k,X,z_n,i)[0])\n",
    "    rish.append(kFoldRidge(k,X,z_n,i)[1])\n",
    "plt.figure()\n",
    "plt.plot(_lams,fish)\n",
    "plt.title(\"mse ridge\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(_lams,rish)\n",
    "plt.title(\"r2 ridge\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = np.delete(X-np.mean(X,axis=0),0,1)\n",
    "z_n_c = z_n -np.mean(z_n)\n",
    "\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "rish=[]\n",
    "\n",
    "for i in _lams:\n",
    "    fish.append(kFoldLassoCV(k,X,z_n,i)[0])\n",
    "    rish.append(kFoldLassoCV(k,X,z_n,i)[1])\n",
    "plt.figure()\n",
    "plt.plot(_lams,fish)\n",
    "plt.title(\"mse kfold lasso\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(_lams,rish)\n",
    "plt.title(\"r2 kfold lasso\")\n",
    "plt.xlabel(\"lambdas\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = 5\n",
    "mpd = 9\n",
    "k=10\n",
    "ms = []\n",
    "rs = []\n",
    "vs = []\n",
    "bs = []\n",
    "\n",
    "for i in range(mpd):\n",
    "    X = Model(x,y,i+1)\n",
    "    Xn,z_nn=Shuffler(X,z_n)\n",
    "    msi=0\n",
    "    rsi = 0\n",
    "    vsi = 0\n",
    "    bsi = 0\n",
    "    \n",
    "#     for l in range(NN):\n",
    "    msi = kFold(k,Xn,z_nn)[0]\n",
    "    rsi = kFold(k,Xn,z_nn)[1]\n",
    "    vsi = kFold(k,Xn,z_nn)[3]\n",
    "    bsi = kFold(k,Xn,z_nn)[4]\n",
    "    ms.append(msi)\n",
    "    rs.append(rsi)\n",
    "    vs.append(vsi)\n",
    "    bs.append(bsi)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(np.arange(mpd)+1,ms)\n",
    "plt.title(\"mse kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(vs)\n",
    "plt.title(\"Var kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "ulp = []\n",
    "for i in range(len(bs)):\n",
    "    ulp.append(bs[i])\n",
    "plt.figure()\n",
    "plt.plot(bs)\n",
    "# plt.plot(ms,'k')\n",
    "plt.title(\"Bias2 kFold for polydegree, 5 reruns\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(mpd)+1,rs)\n",
    "# plt.title(\"r2 kFold for polydegree, 5 reruns\")\n",
    "# plt.show()\n",
    "\n",
    "# print(np.shape(bs),np.shape(vs))\n",
    "# plt.figure()\n",
    "# # plt.plot(np.arange(mpd)+1,ms)\n",
    "# # plt.plot(np.arange(mpd)+1,ms)\n",
    "# bst = [i /90 for i in bs]\n",
    "# plt.plot(bst)\n",
    "# plt.plot(vs)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run NN kFolds to create bias/variance plot\n",
    "#My bias increases again with model complexity\n",
    "NN=5\n",
    "mpd = 15\n",
    "qq = len(z_n)\n",
    "\n",
    "#sort noisy signal\n",
    "z_n_sorted = sorted(z_n)\n",
    "\n",
    "#initialize variables \n",
    "NMSE_k = np.zeros(shape=(NN,mpd)) #MSE stored for each run of kFold\n",
    "model_values = np.zeros(shape=(NN,mpd,qq))\n",
    "bias2 = np.zeros(mpd)\n",
    "var = np.zeros(mpd)\n",
    "bias2_ = np.zeros(shape=(mpd,qq))\n",
    "var_ = np.zeros(shape=(mpd,qq))\n",
    "\n",
    "# betas = np.zeros(shape=(NN,mpd))\n",
    "betas=[]\n",
    "#Run NN kFolds\n",
    "for l in range(NN):\n",
    "    #shuffle here\n",
    "    for i in range(mpd):\n",
    "        X = Model(x,y,i+1)\n",
    "#         model_values[l,i] = kFoldskl(k,X,z_n)[0]\n",
    "        model_values[l,i] = kFold(k,X,z_n)[0]\n",
    "        NMSE_k[l,i] = MSE(model_values[l,i],z_n_sorted)\n",
    "        if l == 0:\n",
    "            betas.append(kFold(k,X,z_n)[1])\n",
    "NMSE = np.mean(NMSE_k,axis=0)\n",
    "\n",
    "#Calculate bias and variance for each polynomial degree\n",
    "for i in range(mpd):\n",
    "    for m in range(qq):\n",
    "        bias2_[i,m] = np.mean(model_values[:,i,m])\n",
    "        var_[i,m] = np.var(model_values[:,i,m])\n",
    "    bias2[i]=np.mean((z_n_sorted-bias2_[i])**2)\n",
    "    var[i] = np.mean(var_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_noise = 8\n",
    "legs = 'MSE','Bias','var','Bias+Var'\n",
    "plt.figure()\n",
    "maxi = 8\n",
    "plt.plot(np.arange(1,maxi+1),NMSE[:maxi],'k')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi],'g')\n",
    "plt.plot(np.arange(1,maxi+1),var[:maxi],'b')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi]+var[:maxi],'ro')\n",
    "plt.xlabel('Ploynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE and bias as functions of polynomial degree, CV')\n",
    "plt.legend(legs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "xp = np.arange(0,1,0.05)\n",
    "yp = np.arange(0,1,0.05)\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "fz = FrankeFunction(xp,yp)\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1,projection='3d')\n",
    "    Xp = Model(xz,yz,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(20,20)\n",
    "    axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "#     axs.plot_surface(xp,yp,fz,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "    axs.set_zlabel('z')\n",
    "    axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "    axs.set_zlim(-0.10,1.40)\n",
    "    axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "xp = np.arange(0,1,0.05)\n",
    "yp = np.arange(0,1,0.05)\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "fz = FrankeFunction(xp,yp)\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1)\n",
    "    Xp = Model(xz,yz,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(20,20)\n",
    "#     plo = zp-fz\n",
    "    plo = np.subtract(zp,fz)\n",
    "#     print(\"a\",zp[0],\"b\",fz[0],\"c\",plo[0])\n",
    "    aa = axs.contourf(plo,cmap=cm.coolwarm)\n",
    "#     axs.plot(plo[0],plo[1])\n",
    "#     axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "#     axs.addaxes(xp)\n",
    "#     axs.set_zlabel('z')\n",
    "#     axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "#     axs.set_zlim(-0.10,1.40)\n",
    "#     axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "#     axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(aa,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_noise = 8\n",
    "legs = 'MSE','Bias','var','Bias+Var'\n",
    "plt.figure()\n",
    "maxi = 9\n",
    "plt.plot(np.arange(1,maxi+1),NMSE[:maxi],'k')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi],'g')\n",
    "plt.plot(np.arange(1,maxi+1),var[:maxi],'b')\n",
    "plt.plot(np.arange(1,maxi+1),bias2[:maxi]+var[:maxi],'ro')\n",
    "plt.xlabel('Ploynomial degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE and bias as functions of polynomial degree, CV')\n",
    "plt.legend(legs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxpd = 5\n",
    "for i in range(maxpd):\n",
    "    print(\"Polynomial degree\", i+1)\n",
    "    X = Model(x,y,i+1)\n",
    "#     X = X-np.mean(X,axis=0)\n",
    "#     X[:,0]=1.\n",
    "#     z_n = z_n -np.mean(z_n)\n",
    "    ztilde = kFold(k,X,z_n)[0]\n",
    "    print(\"OLS\")\n",
    "    print(\"beta\",kFold(k,X,z_n)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "    print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    ztilde = kFoldRidge(k,X,z_n,100.1)[0] #just a random lambda value\n",
    "    print(\"Ridge\")\n",
    "    print(\"beta\",kFoldRidge(k,X,z_n,0.1)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "#     print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    print(\"variance\",VarRidge(X,10000.1))\n",
    "    ztilde = kFoldLassoCV(k,X,z_n,[100.001,100.01])\n",
    "    print(\"Lasso\")\n",
    "    print(\"beta\",kFold(k,X,z_n)[1])\n",
    "    print(\"pm\",1.96*np.sqrt(abs(VarOLS(z_n, ztilde, i,X))))\n",
    "    print(\"variance\",VarOLS(z_n,ztilde,i,X))\n",
    "    print(\"-------------------\")\n",
    "    ### ALL VARIANCES ARE THE SAME, WHY?!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lams = np.logspace(-4,0,10)\n",
    "kFoldLassoCV(k,X,z_n,_lams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPD = np.arange(9)+1#np.arange(9)+1\n",
    "MSE_degree = []\n",
    "\n",
    "\n",
    "#shuffle data before doing the kFold\n",
    "# n = len(X[0,:])\n",
    "# combi = np.c_[X,z_n]\n",
    "# np.random.shuffle(combi)\n",
    "# X, z_n = combi[:,:n], combi[:,n]\n",
    "# X_new,z_n_new = Shuffler(X,z_n)\n",
    "for i in MPD:\n",
    "    X = Model(x,y,i)\n",
    "    \n",
    "    z_out = kFold(k,X,z_n)\n",
    "#     dMSE, dR2 = NoResampling(X,z_n,0)\n",
    "#     dMSE,dR2 = kFoldskl(k,X,z_n)\n",
    "#     print(i,dMSE)\n",
    "    MSE_degree.append(MSE(z_out,z_n_sorted))\n",
    "# MS = MSE(MSE_degree,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MPD+1,MSE_degree)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = np.logspace(-4,5,10)\n",
    "\n",
    "RidgeMSE = np.zeros((len(MPD),len(_lambda)))\n",
    "RidgeR2 = np.zeros((len(MPD),len(_lambda)))\n",
    "\n",
    "\n",
    "\n",
    "for i,mpd in enumerate(MPD):\n",
    "    X = Model(x,y,mpd)\n",
    "    for j,lam in enumerate(_lambda):\n",
    "#         rMSE,rR2 = NoResampling(X,z_n,lam)\n",
    "        z_out = kFoldRidge(k,X,z_n,lam)\n",
    "        RidgeMSE[i][j] = MSE(z_out,z_n_sorted)\n",
    "        RidgeR2[i][j] = R2(z_n_sorted,z_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(_lambda)):\n",
    "    plt.semilogy(MPD,RidgeMSE[:,i],label=_lambda[i])\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(len(MPD)):\n",
    "    plt.loglog(_lambda,RidgeMSE[i,:],label=i+1)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use Scikit-learn as recommended for this\n",
    "kFoldLassoCV(k,X,z_n,_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain = imread('SRTM_data_Norway_1.tif')\n",
    "terr_square = terrain[0:100,0:100]\n",
    "print(np.shape(terr_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra = np.matrix.flatten(terr_square)\n",
    "print(np.shape(terra))\n",
    "# x = np.linspace(0,len(terr_square[0]),100)\n",
    "# y = np.linspace(0,len(terr_square[1]),100)\n",
    "x = np.linspace(0,1,np.sqrt(len(terra)))\n",
    "y=np.linspace(0,1,np.sqrt(len(terra)))\n",
    "x, y = np.meshgrid(x,y,sparse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Model(x,y,5)\n",
    "\n",
    "\n",
    "X_c = X-np.mean(X,axis=0)\n",
    "X_c[:,0]=1.\n",
    "terra_c = terra -np.mean(terra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoResampling(X,terra,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas=[]\n",
    "mpd = 6\n",
    "for i in range(mpd):\n",
    "        X = Model(xp,yp,i+1)\n",
    "        print(np.shape(X))\n",
    "#         model_values[l,i] = kFoldskl(k,X,z_n)[0]\n",
    "#         model_values[l,i] = kFold(k,X,z_n)[0]\n",
    "#         NMSE_k[l,i] = MSE(model_values[l,i],z_n_sorted)\n",
    "#         if l == 0:\n",
    "        betas.append(kFold(k,X,terra)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "# xp = np.arange(0,1,0.05)\n",
    "# yp = np.arange(0,1,0.05)\n",
    "\n",
    "\n",
    "xp = np.linspace(0,1,len(terr_square[0]))\n",
    "yp=np.linspace(0,1,len(terr_square[1]))\n",
    "\n",
    "xp,yp = np.meshgrid(xp,yp,sparse=False)\n",
    "xz,yz = np.meshgrid(xp,yp,sparse=True)\n",
    "pd = 9\n",
    "# fz = FrankeFunction(xp,yp)\n",
    "\n",
    "\n",
    "betas=[]\n",
    "for i in range(pd):\n",
    "        X = Model(xp,yp,i+1)\n",
    "        betas.append(kFold(k,X,terra)[1])\n",
    "\n",
    "for i in range(pd):\n",
    "    axs = fig.add_subplot(3,3,i+1,projection='3d')\n",
    "    Xp = Model(xp,yp,i+1)\n",
    "    zp = Xp@betas[i]\n",
    "    zp.shape=(len(xp),len(yp))\n",
    "    axs.plot_surface(xp,yp,zp,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "#     axs.plot_surface(xp,yp,terr_square,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "    axs.set_xlabel('x')\n",
    "    axs.set_ylabel('y')\n",
    "    axs.set_zlabel('z')\n",
    "    axs.set_title('Polynomial degree = {}'.format(i+1))\n",
    "#     axs.set_zlim(-0.10,1.40)\n",
    "    axs.zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf,shrink=0.5, aspect=5,pad = 0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "axers = fig2.gca(projection='3d')\n",
    "\n",
    "for i in range(pd):\n",
    "    surf = axers.plot_surface(xp,yp,terr_square,cmap=cm.coolwarm,linewidth=0,antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = 5\n",
    "_lams = np.linspace(0.0,1.,10)\n",
    "X = Model(x,y,pd)\n",
    "fish=[]\n",
    "for i in range(len(_lams)):\n",
    "    fish.append(kFoldRidge(k,X,z_n,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Oppgave_a.csv\")\n",
    "# row2 = data.iloc[3]\n",
    "# print(row2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatexPrinter(betas,variances):\n",
    "    for i in range(len(betas)-1):\n",
    "        print(\"$\\\\beta_{{{}}}$ & {} & {} \\\\\\ \".format(i,betas[i],variances[i]))\n",
    "    print(\"$\\\\beta_{{{}}}$ & {} & {} \\\\\\ \\\\hline\".format(i,betas[i],variances[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
