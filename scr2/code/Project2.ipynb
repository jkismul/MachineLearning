{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questions:\n",
    "\"\"\" GERON\n",
    "Note that since instances are picked randomly, some instances may be picked several times per epoch\n",
    "while others may not be picked at all. If you want to be sure that the algorithm goes through every instance\n",
    "at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then\n",
    "shuffle it again, and so on. However, this generally converges more slowly.\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "#mix? sgd for a while, then gd?\n",
    "#include newton raphson too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo:\n",
    "# make more lr and init methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    #importing data set(s)\n",
    "    filename = 'default of credit card clients.xls'\n",
    "    nanDict = {} #this does nothing with this data set\n",
    "    #read file\n",
    "    df = pd.read_excel(filename,header=1,skiprows=0,index_col=0,na_values=nanDict) \n",
    "    #rename last column\n",
    "    df.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "    #Replace nonsensical values in PAY_i columns with 0\n",
    "    for i in [0,2,3,4,5,6]:\n",
    "        col = 'PAY_{}'.format(i)\n",
    "        df[col].replace(to_replace=-2, value = 0, inplace=True)\n",
    "    #shuffle dataset by row\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    # Define features and targets \n",
    "    X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values\n",
    "    y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values\n",
    "    \n",
    "    # Categorical variables to one-hots, setting nonsensical values to 0\n",
    "    onehotencoder1 = OneHotEncoder(categories='auto')\n",
    "    onehotencoder2 = OneHotEncoder(categories='auto',drop='first')\n",
    "\n",
    "    # sets number of elements in onehot vectors automatically from data.\n",
    "    Xt= ColumnTransformer(\n",
    "        [(\"one\", onehotencoder1, [1]),(\"two\", onehotencoder2, [2,3]),],\n",
    "        remainder=\"passthrough\"\n",
    "    ).fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    trainingShare = 0.8\n",
    "    seed  = 1\n",
    "    XTrain, XTest, yTrain, yTest=train_test_split(Xt, y, train_size=trainingShare, \\\n",
    "                                                  test_size = 1-trainingShare,\n",
    "                                                 random_state=seed)\n",
    "    \n",
    "    #scale data, except one-hotted\n",
    "    sc = StandardScaler()\n",
    "    XTrain_fitting = XTrain[:,11:]\n",
    "    XTest_fitting = XTest[:,11:]\n",
    "    #removes mean, scales by std\n",
    "    XTrain_scaler = sc.fit_transform(XTrain_fitting)\n",
    "    XTest_scaler = sc.transform(XTest_fitting)\n",
    "    #puts together the complete model matrix again\n",
    "    XTrain_scaled=np.c_[XTrain[:,:11],XTrain_scaler]\n",
    "    XTest_scaled = np.c_[XTest[:,:11],XTest_scaler]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return XTrain_scaled,XTest_scaled,yTrain,yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s): # can be shortened to: return 1./(1.+np.exp(-s))\n",
    "    out = np.zeros(s.shape)\n",
    "    for i,S in enumerate(s):\n",
    "        out[i]= 1./(1.+np.exp(-S))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(X,theta,y):\n",
    "    return (X.T@(sigmoid(X@theta)-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunction(X,theta,y):\n",
    "    y_pred= sigmoid(X@theta)\n",
    "    y_pred_neg = sigmoid(-X@theta)\n",
    "    cost__ = -y*np.log(y_pred)-(1-y)*np.log(y_pred_neg)\n",
    "    cost_ = np.sum(cost__,axis=0)\n",
    "#     print(cost__)\n",
    "    return cost_/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes two arrays of equal length and calculates the \"Accuracy score\"\n",
    "\"\"\"\n",
    "def Accuracy(t,y):\n",
    "    assert len(t)==len(y), \"y and y_pred dimensions do not match ({},{}).\".format(len(t),len(y))\n",
    "    s = [1 if i else 0 for i in np.equal(t,y)]\n",
    "    return np.sum(s)/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function is pretty much stolen from\n",
    "https://stackoverflow.com/questions/38157972/how-to-implement-mini-batch-gradient-descent-in-python\n",
    "users Ash and dsachar\n",
    "\"\"\"\n",
    "def BatchIterator(X,y,batch_size,shuffle=True):\n",
    "    assert X.shape[0]==y.shape[0], \"X and y dimensions do not match ({},{}).\".format(X.shape[0],y.shape[0])\n",
    "    if shuffle:\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_index in range(0,X.shape[0],batch_size):\n",
    "        end_index = min(start_index+batch_size,X.shape[0])\n",
    "        if shuffle:\n",
    "            batch_indices = indices[start_index:end_index]\n",
    "        else:\n",
    "            batch_indices = slice(start_index,end_index)\n",
    "        yield X[batch_indices],y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(t):# simulated annealing?\n",
    "        t0,t1 = 5,50\n",
    "        return t0/(t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(XTrain,yTrain,XTest,yTest,gd_method='GD',lr_method='const',init_method='rand',tol=1e-5,\n",
    "        max_iters=200,eta=0.00001,N_epochs=200,N_batches=100,Shuffle=True,seed=1234):\n",
    "    #INIT METHODS\n",
    "    np.random.seed(seed)\n",
    "    theta = np.random.randn(XTrain.shape[1],1) #if function is convex, start point wont matter, and this is fine\n",
    "    \n",
    "    #LR METHODS\n",
    "    #     u,s,v = np.linalg.svd(XTrain_scaled.T@XTrain_scaled)\n",
    "    #     eta = 2/np.max(s) # learning rate must be smaller than this in order to converge!  \n",
    "    #evt svd igjen for max gamma\n",
    "\n",
    "    ce_prev = 1e6\n",
    "    ce=1e6+1\n",
    "    \n",
    "    #GD METHODS\n",
    "    if gd_method=='GD':\n",
    "        c = 0\n",
    "        while abs(ce-ce_prev)>tol and c<max_iters:\n",
    "            ce_prev=ce\n",
    "            gradient = Gradient(XTrain,theta,yTrain)\n",
    "            theta -= eta*gradient*0.9999\n",
    "            ce = CostFunction(XTrain,theta,yTrain) #used for convergence check\n",
    "            c+=1\n",
    "    if gd_method=='SGD':\n",
    "        for epoch in range(N_epochs):\n",
    "            for i in range(XTrain.shape[0]):\n",
    "                random_index = np.random.randint(XTrain.shape[0])\n",
    "                xi = XTrain[random_index:random_index+1]\n",
    "                yi = yTrain[random_index:random_index+1]\n",
    "                gradients = Gradient(xi,theta,yi)\n",
    "#                 eta=simulated_annealing(epoch*N_batches+i) bad with or withoud this\n",
    "                theta -=eta*gradients\n",
    "    if gd_method =='SGDmb':\n",
    "        for i in range(N_epochs):\n",
    "            for batch in BatchIterator(XTrain,yTrain,int(XTrain.shape[0]/N_batches)+1,shuffle=Shuffle):\n",
    "                X_batch,y_batch = batch\n",
    "                gradients = Gradient(X_batch,theta,y_batch)\n",
    "                theta -= eta*gradients\n",
    "    if gd_method == 'skl':\n",
    "        sgd_clf = SGDClassifier(random_state=1,loss='log',tol=1e-5,max_iter=500,penalty='none',alpha=0.,eta0=eta,learning_rate='constant',fit_intercept=False)\n",
    "        sgd_clf.fit(XTrain,yTrain.ravel());\n",
    "        theta = sgd_clf.coef_[0,:].reshape(-1,1)\n",
    "        print(sgd_clf.coef_)\n",
    "    #CALC AND OUTPUT ACCURACY\n",
    "    y_pred_ = sigmoid(XTest@theta)\n",
    "    y_pred_[y_pred_>=0.5]=1\n",
    "    y_pred_[y_pred_<0.5]=0\n",
    "    \n",
    "    strings = [\"Accuracy baseline: \",\"Accuracy of {} method: \".format(method)]\n",
    "    print(\"{:26s}   {:5f}\".format(strings[0],Accuracy(yTest,np.zeros((yTest.shape)))))\n",
    "    print(\"{:26s}   {:5f}\".format(strings[1],Accuracy(yTest,y_pred_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data and define methods and initialization seeds to run over\n",
    "XTrain,XTest,yTrain,yTest = ReadData()\n",
    "methods = ['GD','SGD','SGDmb','skl']\n",
    "seeds = [76,9865,75,34,875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed is 76.\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of GD method:       0.811000\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGD method:      0.810833\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGDmb method:    0.810833\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of skl method:      0.808500\n",
      "Seed is 9865.\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of GD method:       0.806500\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGD method:      0.807333\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGDmb method:    0.806500\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of skl method:      0.808500\n",
      "Seed is 75.\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of GD method:       0.809667\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGD method:      0.810000\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGDmb method:    0.809667\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of skl method:      0.808500\n",
      "Seed is 34.\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of GD method:       0.800667\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGD method:      0.800667\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGDmb method:    0.800167\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of skl method:      0.808500\n",
      "Seed is 875.\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of GD method:       0.804500\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGD method:      0.804500\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of SGDmb method:    0.804500\n",
      "Accuracy baseline:           0.777167\n",
      "Accuracy of skl method:      0.808500\n"
     ]
    }
   ],
   "source": [
    "#run regressions\n",
    "for seed in seeds:\n",
    "    print(\"Seed is {}.\".format(seed))\n",
    "    for method in methods:\n",
    "        SGD(XTrain,yTrain,XTest,yTest,gd_method=method,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
