{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import scipy.linalg as scl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define activation/deactivation functions, and shove them into a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1./(1.+np.exp(-s))\n",
    "def sigmoid_backprop(da,x):\n",
    "    sig = sigmoid(x)\n",
    "    return da*sig*(1-sig)\n",
    "def ReLU(s):\n",
    "    return np.maximum(0,s) \n",
    "def ReLU_backprop(da,x):\n",
    "    rel = ReLU(x)\n",
    "    rel[rel>0]=1\n",
    "    return da*rel\n",
    "#tanh, leakyrelu, relu6.. others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dictionaries for activation/deactivation functions (avoids if tests!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {'sigmoid' :sigmoid, 'ReLU':ReLU}\n",
    "deactivation_functions = {'sigmoid' : sigmoid_backprop, 'ReLU': ReLU_backprop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple:\n",
    "    def __init__(\n",
    "            self,\n",
    "            XTrain,\n",
    "            yTrain,\n",
    "            eta,\n",
    "            nodes_per_layer,\n",
    "            activation_per_layer,\n",
    "            epochs = 10000,\n",
    "            seed=99\n",
    "        ):\n",
    "        self.XTrain = XTrain\n",
    "        self.yTrain = yTrain.T\n",
    "        self.seed   = seed\n",
    "        # add final layer, 1 output node, using the sigmoid activation \n",
    "        # (so this NN only works for binary classification)\n",
    "        nodes_per_layer.extend([1])\n",
    "        activation_per_layer.extend(['sigmoid'])\n",
    "        ###\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.activation_per_layer = activation_per_layer\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def initialize(\n",
    "            self\n",
    "        ):\n",
    "        np.random.seed(self.seed)\n",
    "        number_of_layers = len(self.activation_per_layer)\n",
    "        self.nodes_per_layer = np.hstack((self.XTrain.shape[1],self.nodes_per_layer))\n",
    "        self.parameter_values={}\n",
    "        for idx in range(len(self.activation_per_layer)):\n",
    "            layer_idx = idx+1\n",
    "            layer_input_size = self.nodes_per_layer[idx]\n",
    "            layer_output_size = self.nodes_per_layer[idx+1]\n",
    "            np.random.seed(self.seed)\n",
    "            self.parameter_values['W'+str(layer_idx)] = np.random.randn(\n",
    "                                layer_output_size,layer_input_size)*0.1\n",
    "            self.parameter_values['b'+str(layer_idx)] = np.random.randn(\n",
    "                                layer_output_size,1)*0.1\n",
    "    \n",
    "    \n",
    "    def forward_propagation(\n",
    "        self,\n",
    "        X\n",
    "        ):\n",
    "        self.layer_dict = {}\n",
    "        self.a_current = X.T\n",
    "        \n",
    "        for idx in range(len(self.activation_per_layer)):\n",
    "            layer_idx = idx+1\n",
    "            self.a_previous = self.a_current\n",
    "            self.W_current = self.parameter_values[\"W\"+str(layer_idx)]\n",
    "            self.b_current = self.parameter_values[\"b\"+str(layer_idx)]\n",
    "            self.z_current = self.W_current@self.a_previous+self.b_current\n",
    "            self.a_current = activation_functions[self.activation_per_layer[idx]](self.z_current)\n",
    "            self.layer_dict[\"a\"+str(idx)] = self.a_previous\n",
    "            self.layer_dict[\"z\"+str(layer_idx)] = self.z_current\n",
    "        return self.a_current\n",
    "    \n",
    "    def backward_propagation(\n",
    "        self\n",
    "        ):\n",
    "        self.gradient_values={}\n",
    "        m = self.a_previous.shape[1]\n",
    "        da_previous = -(np.divide(self.yTrain,self.a_current)-np.divide(1-self.yTrain,1-self.a_current))\n",
    "\n",
    "        for layer_idx_previous in reversed(range(len(self.activation_per_layer))):\n",
    "            layer_idx_current = layer_idx_previous + 1\n",
    "            da_current = da_previous\n",
    "\n",
    "            a_previous = self.layer_dict[\"a\" + str(layer_idx_previous)]\n",
    "            z_current = self.layer_dict[\"z\" + str(layer_idx_current)]\n",
    "            W_current = self.parameter_values[\"W\" + str(layer_idx_current)]\n",
    "            b_current = self.parameter_values[\"b\" + str(layer_idx_current)]\n",
    "\n",
    "            n = a_previous.shape[1]\n",
    "            dz_current = deactivation_functions[self.activation_per_layer[layer_idx_current-1]](da_current,z_current)\n",
    "            dW_current = dz_current@a_previous.T/n\n",
    "            db_current = np.sum(dz_current,axis=1,keepdims=True)/n\n",
    "            da_previous = W_current.T@dz_current\n",
    "\n",
    "            self.gradient_values[\"dW\" + str(layer_idx_current)] = dW_current\n",
    "            self.gradient_values[\"db\" + str(layer_idx_current)] = db_current \n",
    "            \n",
    "    ### COST STUFF\n",
    "    def cross_entropy(\n",
    "        self,\n",
    "        y_pred,\n",
    "        y_real\n",
    "        ):\n",
    "        cost = (-1./y_pred.shape[1])*((y_real@(np.log(y_pred).T)+(1-y_real)@(np.log(1-y_pred)).T))\n",
    "        return np.squeeze(cost[:,np.newaxis]) \n",
    "    \n",
    "    def accuracy(\n",
    "        self,\n",
    "        y_pred,\n",
    "        y_real\n",
    "        ):\n",
    "        if y_pred.shape[0] != 1:\n",
    "            y_pred = y_pred.reshape(1,y_pred.shape[0])\n",
    "        y_real = y_real.reshape(y_pred.shape)\n",
    "        y_pred_classed = np.copy(y_pred)\n",
    "        y_pred_classed[y_pred_classed>0.5]=1\n",
    "        y_pred_classed[y_pred_classed<=0.5]=0\n",
    "        return (y_pred_classed==y_real).all(axis=0).mean()\n",
    "    \n",
    "    ## UPDATE WEIGHTS AND BIASES\n",
    "    def update(\n",
    "        self\n",
    "        ):\n",
    "        for layer_idx in range(len(activation_per_layer)):\n",
    "            layer_idx +=1\n",
    "            self.parameter_values[\"W\" + str(layer_idx)] -= self.eta * self.gradient_values[\"dW\" + str(layer_idx)]        \n",
    "            self.parameter_values[\"b\" + str(layer_idx)] -= self.eta * self.gradient_values[\"db\" + str(layer_idx)]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def train(\n",
    "        self\n",
    "        ):\n",
    "        self.initialize()\n",
    "        self.cost_history = []\n",
    "        self.accuracy_history = []\n",
    "#         y_things=[]\n",
    "        for i in range(self.epochs):\n",
    "            if i%1000==0:\n",
    "                print(\"Calculating epoch \",i)\n",
    "            self.forward_propagation(self.XTrain)\n",
    "            cost = self.cross_entropy(self.a_current, self.yTrain)\n",
    "            self.cost_history.append(cost)\n",
    "            accuracy = self.accuracy(self.a_current, self.yTrain)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "#             y_things.append(self.A_current)\n",
    "            self.backward_propagation()\n",
    "            self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_per_layer = [100,50,70,80]\n",
    "activation_per_layer = ['sigmoid','ReLU','ReLU','ReLU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on skl moon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating epoch  0\n",
      "Calculating epoch  1000\n",
      "Calculating epoch  2000\n",
      "Calculating epoch  3000\n",
      "Calculating epoch  4000\n",
      "Calculating epoch  5000\n",
      "Calculating epoch  6000\n",
      "Calculating epoch  7000\n",
      "Calculating epoch  8000\n",
      "Calculating epoch  9000\n"
     ]
    }
   ],
   "source": [
    "Moons = simple(X_train_moons,y_train_moons,0.01,nodes_per_layer,activation_per_layer)\n",
    "Moons.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.85 \n",
      "Zero set accuracy: 0.54 \n"
     ]
    }
   ],
   "source": [
    "Y_test_hat_moons = Moons.forward_propagation(X_test_moons)\n",
    "acc_test_moons = Moons.accuracy(Y_test_hat_moons, y_test_moons)\n",
    "print(\"Test set accuracy: {:.2f} \".format(acc_test_moons))\n",
    "zerot_moons = np.zeros(y_test_moons.shape[0])\n",
    "acc_test_moons2 = Moons.accuracy(y_test_moons, zerot_moons)\n",
    "print(\"Zero set accuracy: {:.2f} \".format(acc_test_moons2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on credit card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    #importing data set(s)\n",
    "    filename = 'default of credit card clients.xls'\n",
    "    nanDict = {} #this does nothing with this data set\n",
    "    #read file\n",
    "    df = pd.read_excel(filename,header=1,skiprows=0,index_col=0,na_values=nanDict) \n",
    "    #rename last column\n",
    "    df.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "    #Replace nonsensical values in PAY_i columns with 0\n",
    "    for i in [0,2,3,4,5,6]:\n",
    "        col = 'PAY_{}'.format(i)\n",
    "        df[col].replace(to_replace=-2, value = 0, inplace=True)\n",
    "    #shuffle dataset by row\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    # Define features and targets \n",
    "    X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values\n",
    "    y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values\n",
    "    \n",
    "    # Categorical variables to one-hots, setting nonsensical values to 0\n",
    "    onehotencoder1 = OneHotEncoder(categories='auto')\n",
    "    onehotencoder2 = OneHotEncoder(categories='auto',drop='first')\n",
    "\n",
    "    # sets number of elements in onehot vectors automatically from data.\n",
    "    Xt= ColumnTransformer(\n",
    "        [(\"one\", onehotencoder1, [1]),(\"two\", onehotencoder2, [2,3]),],\n",
    "        remainder=\"passthrough\"\n",
    "    ).fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    trainingShare = 0.5\n",
    "    seed  = 1\n",
    "    XTrain, XTest, yTrain, yTest=train_test_split(Xt, y, train_size=trainingShare, \\\n",
    "                                                  test_size = 1-trainingShare,\n",
    "                                                 random_state=seed, stratify = y)\n",
    "    \n",
    "    #scale data, except one-hotted\n",
    "    sc = StandardScaler()\n",
    "    XTrain_fitting = XTrain[:,11:]\n",
    "    XTest_fitting = XTest[:,11:]\n",
    "    #removes mean, scales by std\n",
    "    XTrain_scaler = sc.fit_transform(XTrain_fitting)\n",
    "    XTest_scaler = sc.transform(XTest_fitting)\n",
    "    #puts together the complete model matrix again\n",
    "    XTrain_scaled=np.c_[XTrain[:,:11],XTrain_scaler]\n",
    "    XTest_scaled = np.c_[XTest[:,:11],XTest_scaler]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return XTrain_scaled,XTest_scaled,yTrain,yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "XTrain_cc,XTest_cc,yTrain_cc,yTest_cc = ReadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating epoch  0\n",
      "Calculating epoch  1000\n",
      "Calculating epoch  2000\n",
      "Calculating epoch  3000\n",
      "Calculating epoch  4000\n",
      "Calculating epoch  5000\n",
      "Calculating epoch  6000\n",
      "Calculating epoch  7000\n",
      "Calculating epoch  8000\n",
      "Calculating epoch  9000\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "CreditCard = simple(XTrain_cc,yTrain_cc,0.01,nodes_per_layer,activation_per_layer)\n",
    "CreditCard.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.81 \n",
      "Zero set accuracy: 0.78 \n"
     ]
    }
   ],
   "source": [
    "#find accuracy scores\n",
    "Y_test_hat_cc = CreditCard.forward_propagation(XTest_cc)\n",
    "# # Accuracy achieved on the test set\n",
    "acc_test_cc = CreditCard.accuracy(Y_test_hat_cc, yTest_cc)\n",
    "print(\"Test set accuracy: {:.2f} \".format(acc_test_cc))\n",
    "zerot = np.zeros(yTest_cc.shape[0])\n",
    "acc_test_cc2 = CreditCard.accuracy(np.transpose(yTest_cc.reshape((yTest_cc.shape[0], 1))), np.transpose(zerot.reshape((zerot.shape[0], 1))))\n",
    "\n",
    "acc_test_cc2 = CreditCard.accuracy(yTest_cc,zerot)\n",
    "print(\"Zero set accuracy: {:.2f} \".format(acc_test_cc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
